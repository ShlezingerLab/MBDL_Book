{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "import torch, numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as Data\n",
    "\n",
    "\n",
    "from plots import plot_admm_vs_admm_1d_reconstruction\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "torch.set_default_dtype(torch.float64)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data\n",
    "Next, we generate a data set $\\mathcal{D}=\\{(\\boldsymbol{s}_t, \\boldsymbol{x}_t)\\}_{t=1}^{n_t}$, where each $\\boldsymbol{s}_t$ has $m=200$ entries, out of which at most $k=4$ are non-zero, while $\\boldsymbol{x}_t$ has $n=150$ entires and is obtained via\n",
    "\\begin{equation}\n",
    "\\boldsymbol{x}_t = \\boldsymbol{H}\\boldsymbol{s}_t + \\boldsymbol{w}_t,\n",
    "\\end{equation}\n",
    "with $\\boldsymbol{w}_t$ being i.i.d. Gaussian noise.\n",
    "\n",
    "To that aim, we generate a dedicated class inheriting Dataset to get the data samples, and to allow each tuple to be comprised of both $(\\boldsymbol{s}_t, \\boldsymbol{x}_t)$ as well as $\\boldsymbol{H}$."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "class SimulatedData(Data.Dataset):\n",
    "    def __init__(self, x, H, s):\n",
    "        self.x = x\n",
    "        self.s = s\n",
    "        self.H = H\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.x.shape[1]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.x[:, idx]\n",
    "        H = self.H\n",
    "        s = self.s[:, idx]\n",
    "        return x, H, s"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The next function creates a dataset.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "def create_data_set(H, n, m, k, N=1000, batch_size=512, snr=30, noise_dev=0.5):\n",
    "    # Initialization\n",
    "\n",
    "    x = torch.zeros(n, N)\n",
    "    s = torch.zeros(m, N)\n",
    "    # Create signals\n",
    "    for i in range(N):\n",
    "        # Create a sparsed signal s\n",
    "        index_k = np.random.choice(m, k, replace=False)\n",
    "        peaks = noise_dev * np.random.randn(k)\n",
    "\n",
    "        s[index_k, i] = torch.from_numpy(peaks).to(s)\n",
    "\n",
    "        # X = Hs+w\n",
    "        x[:, i] = H @ s[:, i] + 0.01 * np.random.randn(n)\n",
    "\n",
    "    simulated = SimulatedData(x=x, H=H, s=s)\n",
    "    data_loader = Data.DataLoader(dataset=simulated, batch_size=batch_size, shuffle=True)\n",
    "    return data_loader"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, we generate the data set\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "n, m, k = 150, 200, 4\n",
    "\n",
    "# Measurement matrix\n",
    "H = torch.randn(n, m)\n",
    "H /= torch.norm(H, dim=0)\n",
    "\n",
    "train_loader = create_data_set(H, n=n, m=m, k=k, N=1000)\n",
    "\n",
    "test_loader = create_data_set(H, n=n, m=m, k=k, N=1000)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Vanilla ADMM implementation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vanilla_admm(x, H, lambda_=12.5, mu=0.00005, rho=0.01, max_itr=300, eps=10 ** -5):\n",
    "    proj = torch.nn.Softshrink(rho / (2 * lambda_))\n",
    "\n",
    "    # initial estimate\n",
    "    s = torch.zeros((H.shape[1]))\n",
    "    u = torch.zeros((H.shape[1]))\n",
    "    v = torch.zeros((H.shape[1]))\n",
    "\n",
    "    # left_term = (H^TH+2λI)^-1 2*lambda or rho?\n",
    "    left_term = torch.linalg.inv(H.T @ H + rho * torch.eye(H.shape[1]))\n",
    "\n",
    "    recovery_errors = []\n",
    "    for k in range(max_itr):\n",
    "        s_prev, v_prev, u_prev = s, v, u\n",
    "\n",
    "        # Update s_k+1 = ((H^T)H+2λI)^−1(H^T x+2λ(vk−uk)).\n",
    "\n",
    "        right_term = H.T @ x + rho * (v_prev - u_prev)\n",
    "        s = left_term @ right_term\n",
    "\n",
    "        # Update vk+1 = prox_(1/2λϕ)(sk+1 + uk)\n",
    "        v = proj(s + u_prev)\n",
    "\n",
    "        # Update uk+1 = uk + μ (sk+1 − vk+1).\n",
    "\n",
    "        u = u_prev + mu * (s - v)\n",
    "\n",
    "        # # cease if convergence achieved\n",
    "        if torch.sum(torch.abs(s - s_prev)) <= eps:\n",
    "            break\n",
    "\n",
    "        # save recovery error\n",
    "        recovery_errors.append(torch.sum((torch.matmul(H, s) - x) ** 2))\n",
    "\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "1D Model Based ADMM implementation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "EPSILON = 10 ** -2\n",
    "\n",
    "class LADMM_Model_1D(nn.Module):\n",
    "    def __init__(self, n, m, max_iterations=1000, rho=0.01, H=None, lambda_=12.5, mu=0.00005, epsilon=EPSILON):\n",
    "        super(LADMM_Model_1D, self).__init__()\n",
    "        self.n, self.m = n, m\n",
    "        self.H = H\n",
    "\n",
    "        # admm(x, H, lambda_=12.5, mu=0.00005, rho=0.01, max_itr=300, eps=10 ** -5):\n",
    "\n",
    "        # Initialization of 1 dimensional parameter\n",
    "        self.rho = nn.Parameter(torch.ones(1) * rho, requires_grad=True)\n",
    "        self.lambda_ = nn.Parameter(torch.ones(1) * lambda_, requires_grad=True)\n",
    "        self.mu = nn.Parameter(torch.ones(1) * mu, requires_grad=True)\n",
    "\n",
    "        self.max_iteration = max_iterations\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def _shrink(self, s, beta, rho):\n",
    "        return beta * F.softshrink(s / beta, lambd=rho)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x:\n",
    "        Returns:\n",
    "        \"\"\"\n",
    "\n",
    "        s_prev = torch.zeros(x.shape[0], self.H.shape[1])\n",
    "        u_prev = torch.zeros((x.shape[0], self.H.shape[1]))\n",
    "        v_prev = torch.zeros((x.shape[0], self.H.shape[1]))\n",
    "\n",
    "        #################### Iteration 0 ####################\n",
    "\n",
    "        left_term = torch.linalg.inv(H.T @ H + self.rho * torch.eye(H.shape[1]))\n",
    "\n",
    "        right_term = (H.T @ x.T).T + self.rho * (v_prev - u_prev)\n",
    "\n",
    "        s = (left_term @ right_term.T).T\n",
    "        v = self._shrink(s + u_prev, self.rho / (2 * self.lambda_), rho=self.rho.item())\n",
    "        u = u_prev + self.mu * (s - v)\n",
    "\n",
    "        ######################################################\n",
    "        iteration = 0\n",
    "\n",
    "        # Notice the stopping condition isn't fixed K iterations\n",
    "        while (torch.norm(s_prev.detach() - s.detach()).item() > self.epsilon) and (iteration < self.max_iteration):\n",
    "            s_prev, v_prev, u_prev = s, v, u\n",
    "\n",
    "            # left_term = (H^TH+2λI)^-1 2*lambda or rho?\n",
    "            left_term = torch.linalg.inv(H.T @ H + self.rho * torch.eye(H.shape[1]))\n",
    "\n",
    "            right_term = (H.T @ x.T).T + self.rho * (v_prev - u_prev)\n",
    "\n",
    "            # Update s_k+1 = ((H^T)H+2λI)^−1(H^T x+2λ(vk−uk)).\n",
    "            s = (left_term @ right_term.T).T\n",
    "\n",
    "            # Update vk+1 = prox_(1/2λϕ)(sk+1 + uk)\n",
    "            v = self._shrink(s + u_prev, self.rho / (2 * self.lambda_), rho=self.rho.item())\n",
    "\n",
    "            # Update uk+1 = uk + μ (sk+1 − vk+1).\n",
    "            u = u_prev + self.mu * (s - v)\n",
    "            iteration += 1\n",
    "\n",
    "        print(\"Batch total iterations: {0}, parameters: mu:{1} lambda:{2} rho:{3}\".format(iteration, self.mu.item(),\n",
    "                                                                                          self.lambda_.item(),\n",
    "                                                                                          self.rho.item()))\n",
    "        return s"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train and Validation\n",
    "\n",
    "For training, we use SGDM with learning rate scheduler and the $\\ell_2$ loss"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "def train(model, train_loader, valid_loader, num_epochs=60):\n",
    "    \"\"\"Train a network.\n",
    "    Returns:\n",
    "        loss_test {numpy} -- loss function values on test set\n",
    "    \"\"\"\n",
    "    # Initialization\n",
    "    optimizer = torch.optim.SGD(\n",
    "        model.parameters(),\n",
    "        lr=5e-05,\n",
    "        momentum=0.9,\n",
    "        weight_decay=0,\n",
    "    )\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.1)\n",
    "    loss_train = np.zeros((num_epochs,))\n",
    "    loss_test = np.zeros((num_epochs,))\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for step, (b_x, b_H, b_s) in enumerate(train_loader):\n",
    "\n",
    "            s_hat = model(b_x)\n",
    "            loss = F.mse_loss(s_hat, b_s, reduction=\"sum\")\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            model.zero_grad()\n",
    "            train_loss += loss.data.item()\n",
    "\n",
    "        # Aggregate loss\n",
    "        loss_train[epoch] = train_loss / len(train_loader.dataset)\n",
    "        scheduler.step()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        for step, (b_x, b_H, b_s) in enumerate(valid_loader):\n",
    "            s_hat = model(b_x)\n",
    "            test_loss += F.mse_loss(s_hat, b_s, reduction=\"sum\").data.item()\n",
    "        loss_test[epoch] = test_loss / len(valid_loader.dataset)\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(\"Epoch %d, Train loss %.8f, Validation loss %.8f\" % (epoch, loss_train[epoch], loss_test[epoch]))\n",
    "\n",
    "    return loss_test, b_x, b_s"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following functions apply 1D L-ADMM and ADMM for given data sets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "def admm_1d_apply(train_loader, test_loader, max_iterations, H):\n",
    "    n = H.shape[1]\n",
    "    m = H.shape[1]\n",
    "\n",
    "    ladmm = LADMM_Model_1D(n=n, m=m, max_iterations=max_iterations, H=H)\n",
    "\n",
    "    loss_test, b_x, b_s = train(ladmm, train_loader, test_loader)\n",
    "    error = loss_test[-1]\n",
    "\n",
    "    return error, ladmm, b_x, b_s\n",
    "\n",
    "\n",
    "def admm_apply(test_loader, T, H):\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for step, (x, _, s) in enumerate(test_loader.dataset):\n",
    "        s_hat = vanilla_admm(x=x, H=H, max_itr=T)\n",
    "        loss += F.mse_loss(s_hat, s, reduction=\"sum\").data.item()\n",
    "\n",
    "    return loss / len(test_loader.dataset)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch total iterations: 4, parameters: mu:5e-05 lambda:12.5 rho:0.01\n",
      "Batch total iterations: 4, parameters: mu:5.299065391925642e-05 lambda:12.499999760703504 rho:0.010418078812265781\n",
      "Batch total iterations: 5, parameters: mu:5.880947601420902e-05 lambda:12.499999295112985 rho:0.011218523688541726\n",
      "Batch total iterations: 5, parameters: mu:5.880947601420902e-05 lambda:12.499999295112985 rho:0.011218523688541726\n",
      "Epoch 0, Train loss 0.30321073, Validation loss 0.30565641\n",
      "Batch total iterations: 4, parameters: mu:5.880947601420902e-05 lambda:12.499999295112985 rho:0.011218523688541726\n",
      "Batch total iterations: 5, parameters: mu:6.786791201081621e-05 lambda:12.499998570295917 rho:0.0124329340377923\n",
      "Batch total iterations: 5, parameters: mu:8.150106249727772e-05 lambda:12.499997479383318 rho:0.01403182476719406\n",
      "Batch total iterations: 5, parameters: mu:8.150106249727772e-05 lambda:12.499997479383318 rho:0.01403182476719406\n",
      "Batch total iterations: 5, parameters: mu:8.150106249727772e-05 lambda:12.499997479383318 rho:0.01403182476719406\n",
      "Batch total iterations: 5, parameters: mu:0.00010100835429067744 lambda:12.499995918373779 rho:0.016085136720816864\n",
      "Batch total iterations: 6, parameters: mu:0.00012806714702837753 lambda:12.499993752927944 rho:0.018659394588002223\n",
      "Batch total iterations: 6, parameters: mu:0.00012806714702837753 lambda:12.499993752927944 rho:0.018659394588002223\n",
      "Batch total iterations: 6, parameters: mu:0.00012806714702837753 lambda:12.499993752927944 rho:0.018659394588002223\n",
      "Batch total iterations: 6, parameters: mu:0.00016787588210368087 lambda:12.499990566694782 rho:0.021894767491016723\n",
      "Batch total iterations: 7, parameters: mu:0.00022469684316082662 lambda:12.49998601818515 rho:0.025908980352634576\n",
      "Batch total iterations: 7, parameters: mu:0.00022469684316082662 lambda:12.49998601818515 rho:0.025908980352634576\n",
      "Batch total iterations: 7, parameters: mu:0.00022469684316082662 lambda:12.49998601818515 rho:0.025908980352634576\n",
      "Batch total iterations: 8, parameters: mu:0.00031138130029437314 lambda:12.499979076990504 rho:0.031020000160491713\n",
      "Batch total iterations: 128, parameters: mu:0.00044399449282925144 lambda:12.499968452730872 rho:0.037494770593485244\n",
      "Batch total iterations: 103, parameters: mu:0.00044399449282925144 lambda:12.499968452730872 rho:0.037494770593485244\n",
      "Batch total iterations: 117, parameters: mu:0.00044399449282925144 lambda:12.499968452730872 rho:0.037494770593485244\n",
      "Batch total iterations: 453, parameters: mu:0.001582015366249969 lambda:12.499874941629825 rho:0.0713661093435346\n",
      "Batch total iterations: 291, parameters: mu:0.0033733392888223323 lambda:12.499718486238695 rho:0.11481277957931627\n",
      "Batch total iterations: 277, parameters: mu:0.0033733392888223323 lambda:12.499718486238695 rho:0.11481277957931627\n"
     ]
    }
   ],
   "source": [
    "max_iter, epochs = 1000, 60\n",
    "\n",
    "# Train and apply L-ADMM One Parameter with T iterations / layers. b_x, b_s are a batch from the validation set\n",
    "ladmm_mse, admm1d_model, b_x, b_s = admm_1d_apply(train_loader, test_loader, max_iter, H)\n",
    "\n",
    "admm_mse = admm_apply(test_loader, max_iter, H)\n",
    "\n",
    "######################### Visuallization #########################\n",
    "b_x, s_gt = b_x[0], b_s[0]\n",
    "s_hat_ladmm = admm1d_model(b_x)\n",
    "s_hat_ladmm = s_hat_ladmm.detach().numpy()[0]\n",
    "\n",
    "s_hat_admm = vanilla_admm(x=b_x, H=H, max_itr=max_iter)\n",
    "\n",
    "plot_admm_vs_admm_1d_reconstruction(s_hat_admm=s_hat_admm,\n",
    "                                  s_hat_ladmm=s_hat_ladmm, max_iter=max_iter,s_gt = s_gt, epochs=epochs)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_admm_vs_admm_1d_reconstruction(s_hat_admm=s_hat_admm,\n",
    "                                    s_hat_ladmm=s_hat_ladmm,\n",
    "                                    max_iter=max_iter, s_gt = s_gt, epochs=epochs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
