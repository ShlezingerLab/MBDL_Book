{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import torch, numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "from utills import create_data_set, plot_admm_vs_admm_1d_reconstruction\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "torch.set_default_dtype(torch.float64)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following createss a sparse signals databases. each database comprised of N signal matrix with each signal\n",
    "is 200x1 vector which is k-sparse signal"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "n, m, k = 150, 200, 4\n",
    "\n",
    "# Measurement matrix\n",
    "H = torch.randn(n, m)\n",
    "H /= torch.norm(H, dim=0)\n",
    "\n",
    "train_loader = create_data_set(H, n=n, m=m, k=k, N=1000)\n",
    "\n",
    "test_loader = create_data_set(H, n=n, m=m, k=k, N=1000)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Vanilla ADMM implementation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vanilla_admm(x, H, lambda_=12.5, mu=0.00005, rho=0.01, max_itr=300, eps=10 ** -5):\n",
    "    proj = torch.nn.Softshrink(rho / (2 * lambda_))\n",
    "\n",
    "    # initial estimate\n",
    "    s = torch.zeros((H.shape[1]))\n",
    "    u = torch.zeros((H.shape[1]))\n",
    "    v = torch.zeros((H.shape[1]))\n",
    "\n",
    "    # left_term = (H^TH+2λI)^-1 2*lambda or rho?\n",
    "    left_term = torch.linalg.inv(H.T @ H + rho * torch.eye(H.shape[1]))\n",
    "\n",
    "    recovery_errors = []\n",
    "    for k in range(max_itr):\n",
    "        s_prev, v_prev, u_prev = s, v, u\n",
    "\n",
    "        # Update s_k+1 = ((H^T)H+2λI)^−1(H^T x+2λ(vk−uk)).\n",
    "\n",
    "        right_term = H.T @ x + rho * (v_prev - u_prev)\n",
    "        s = left_term @ right_term\n",
    "\n",
    "        # Update vk+1 = prox_(1/2λϕ)(sk+1 + uk)\n",
    "        v = proj(s + u_prev)\n",
    "\n",
    "        # Update uk+1 = uk + μ (sk+1 − vk+1).\n",
    "\n",
    "        u = u_prev + mu * (s - v)\n",
    "\n",
    "        # # cease if convergence achieved\n",
    "        if torch.sum(torch.abs(s - s_prev)) <= eps:\n",
    "            break\n",
    "\n",
    "        # save recovery error\n",
    "        recovery_errors.append(torch.sum((torch.matmul(H, s) - x) ** 2))\n",
    "\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Model Based ADMM implementation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "EPSILON = 10 ** -2\n",
    "\n",
    "class LADMM_Model_1D(nn.Module):\n",
    "    def __init__(self, n, m, max_iterations=1000, rho=0.01, H=None, lambda_=12.5, mu=0.00005, epsilon=EPSILON):\n",
    "        super(LADMM_Model_1D, self).__init__()\n",
    "        self.n, self.m = n, m\n",
    "        self.H = H\n",
    "\n",
    "        # admm(x, H, lambda_=12.5, mu=0.00005, rho=0.01, max_itr=300, eps=10 ** -5):\n",
    "\n",
    "        # Initialization of 1 dimensional parameter\n",
    "        self.rho = nn.Parameter(torch.ones(1) * rho, requires_grad=True)\n",
    "        self.lambda_ = nn.Parameter(torch.ones(1) * lambda_, requires_grad=True)\n",
    "        self.mu = nn.Parameter(torch.ones(1) * mu, requires_grad=True)\n",
    "\n",
    "        self.max_iteration = max_iterations\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def _shrink(self, s, beta, rho):\n",
    "        return beta * F.softshrink(s / beta, lambd=rho)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x:\n",
    "        Returns:\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        s_prev = torch.zeros(x.shape[0], self.H.shape[1])\n",
    "        u_prev = torch.zeros((x.shape[0], self.H.shape[1]))\n",
    "        v_prev = torch.zeros((x.shape[0], self.H.shape[1]))\n",
    "\n",
    "        #################### Iteration 0 ####################\n",
    "\n",
    "        left_term = torch.linalg.inv(H.T @ H + self.rho * torch.eye(H.shape[1]))\n",
    "\n",
    "        right_term = (H.T @ x.T).T + self.rho * (v_prev - u_prev)\n",
    "\n",
    "        s = (left_term @ right_term.T).T\n",
    "        v = self._shrink(s + u_prev, self.rho / (2 * self.lambda_), rho=self.rho.item())\n",
    "        u = u_prev + self.mu * (s - v)\n",
    "\n",
    "        ######################################################\n",
    "        iteration = 0\n",
    "\n",
    "        # Notice the stopping condition isn't fixed K iterations\n",
    "        while (torch.norm(s_prev.detach() - s.detach()).item() > self.epsilon) and (iteration < self.max_iteration):\n",
    "            s_prev, v_prev, u_prev = s, v, u\n",
    "\n",
    "            # left_term = (H^TH+2λI)^-1 2*lambda or rho?\n",
    "            left_term = torch.linalg.inv(H.T @ H + self.rho * torch.eye(H.shape[1]))\n",
    "\n",
    "            right_term = (H.T @ x.T).T + self.rho * (v_prev - u_prev)\n",
    "\n",
    "            # Update s_k+1 = ((H^T)H+2λI)^−1(H^T x+2λ(vk−uk)).\n",
    "            s = (left_term @ right_term.T).T\n",
    "\n",
    "            # Update vk+1 = prox_(1/2λϕ)(sk+1 + uk)\n",
    "            v = self._shrink(s + u_prev, self.rho / (2 * self.lambda_), rho=self.rho.item())\n",
    "\n",
    "            # Update uk+1 = uk + μ (sk+1 − vk+1).\n",
    "            u = u_prev + self.mu * (s - v)\n",
    "            iteration += 1\n",
    "\n",
    "        print(\"Batch total iterations: {0}, parameters: mu:{1} lambda:{2} rho:{3}\".format(iteration, self.mu.item(),\n",
    "                                                                                          self.lambda_.item(),\n",
    "                                                                                          self.rho.item()))\n",
    "        return s"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The function trains a machine learning model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def train(model, train_loader, valid_loader, num_epochs=60):\n",
    "    \"\"\"Train a network.\n",
    "    Returns:\n",
    "        loss_test {numpy} -- loss function values on test set\n",
    "    \"\"\"\n",
    "    # Initialization\n",
    "    optimizer = torch.optim.SGD(\n",
    "        model.parameters(),\n",
    "        lr=5e-05,\n",
    "        momentum=0.9,\n",
    "        weight_decay=0,\n",
    "    )\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.1)\n",
    "    loss_train = np.zeros((num_epochs,))\n",
    "    loss_test = np.zeros((num_epochs,))\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for step, (b_x, b_H, b_s) in enumerate(train_loader):\n",
    "\n",
    "            s_hat = model(b_x)\n",
    "            loss = F.mse_loss(s_hat, b_s, reduction=\"sum\")\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            model.zero_grad()\n",
    "            train_loss += loss.data.item()\n",
    "\n",
    "        # Aggregate loss\n",
    "        loss_train[epoch] = train_loss / len(train_loader.dataset)\n",
    "        scheduler.step()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        for step, (b_x, b_H, b_s) in enumerate(valid_loader):\n",
    "            s_hat = model(b_x)\n",
    "            test_loss += F.mse_loss(s_hat, b_s, reduction=\"sum\").data.item()\n",
    "        loss_test[epoch] = test_loss / len(valid_loader.dataset)\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(\"Epoch %d, Train loss %.8f, Validation loss %.8f\" % (epoch, loss_train[epoch], loss_test[epoch]))\n",
    "\n",
    "    return loss_test, b_x, b_s"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The apply function perform L-ADMM or ADMM upon the whole sparse signal database and aggregate the loss."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def admm_1d_apply(train_loader, test_loader, max_iterations, H):\n",
    "    n = H.shape[1]\n",
    "    m = H.shape[1]\n",
    "\n",
    "    ladmm = LADMM_Model_1D(n=n, m=m, max_iterations=max_iterations, H=H)\n",
    "\n",
    "    loss_test, b_x, b_s = train(ladmm, train_loader, test_loader)\n",
    "    error = loss_test[-1]\n",
    "\n",
    "    return error, ladmm, b_x, b_s\n",
    "\n",
    "\n",
    "def admm_apply(test_loader, T, H):\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for step, (x, _, s) in enumerate(test_loader.dataset):\n",
    "        s_hat = vanilla_admm(x=x, H=H, max_itr=T)\n",
    "        loss += F.mse_loss(s_hat, s, reduction=\"sum\").data.item()\n",
    "\n",
    "    return loss / len(test_loader.dataset)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch total iterations: 4, parameters: mu:5e-05 lambda:12.5 rho:0.01\n",
      "Batch total iterations: 4, parameters: mu:5.298389142165073e-05 lambda:12.499999761252216 rho:0.010417189157524355\n",
      "Batch total iterations: 5, parameters: mu:5.884584478711163e-05 lambda:12.499999292220467 rho:0.01123078291634062\n",
      "Batch total iterations: 4, parameters: mu:5.884584478711163e-05 lambda:12.499999292220467 rho:0.01123078291634062\n",
      "Epoch 0, Train loss 0.30019529, Validation loss 0.29377592\n",
      "Batch total iterations: 5, parameters: mu:5.884584478711163e-05 lambda:12.499999292220467 rho:0.01123078291634062\n",
      "Batch total iterations: 5, parameters: mu:6.88628106020087e-05 lambda:12.499998490702433 rho:0.012437596668718683\n",
      "Batch total iterations: 5, parameters: mu:8.347796573597211e-05 lambda:12.499997321237489 rho:0.014040887751553384\n",
      "Batch total iterations: 5, parameters: mu:8.347796573597211e-05 lambda:12.499997321237489 rho:0.014040887751553384\n",
      "Batch total iterations: 5, parameters: mu:8.347796573597211e-05 lambda:12.499997321237489 rho:0.014040887751553384\n",
      "Batch total iterations: 5, parameters: mu:0.00010404305022942603 lambda:12.499995675616438 rho:0.016113354429012114\n",
      "Batch total iterations: 6, parameters: mu:0.00013199756282450723 lambda:12.499993438580358 rho:0.018701848366615814\n",
      "Batch total iterations: 6, parameters: mu:0.00013199756282450723 lambda:12.499993438580358 rho:0.018701848366615814\n",
      "Batch total iterations: 6, parameters: mu:0.00013199756282450723 lambda:12.499993438580358 rho:0.018701848366615814\n",
      "Batch total iterations: 6, parameters: mu:0.0001733949734230996 lambda:12.499990125417849 rho:0.021997766046810002\n",
      "Batch total iterations: 7, parameters: mu:0.00023113806646666915 lambda:12.499985503432022 rho:0.02604174991368443\n",
      "Batch total iterations: 7, parameters: mu:0.00023113806646666915 lambda:12.499985503432022 rho:0.02604174991368443\n",
      "Batch total iterations: 7, parameters: mu:0.00023113806646666915 lambda:12.499985503432022 rho:0.02604174991368443\n",
      "Batch total iterations: 8, parameters: mu:0.00031866619650528053 lambda:12.49997849516466 rho:0.03117890236140443\n",
      "Batch total iterations: 137, parameters: mu:0.00045390686867997763 lambda:12.499967662196015 rho:0.037734721843364495\n",
      "Batch total iterations: 104, parameters: mu:0.00045390686867997763 lambda:12.499967662196015 rho:0.037734721843364495\n",
      "Batch total iterations: 146, parameters: mu:0.00045390686867997763 lambda:12.499967662196015 rho:0.037734721843364495\n",
      "Batch total iterations: 431, parameters: mu:0.0017800632921709124 lambda:12.4998579243913 rho:0.07681624894638203\n",
      "Batch total iterations: 265, parameters: mu:0.003588622632300269 lambda:12.499701221377165 rho:0.1217708332248737\n",
      "Batch total iterations: 257, parameters: mu:0.003588622632300269 lambda:12.499701221377165 rho:0.1217708332248737\n",
      "Batch total iterations: 258, parameters: mu:0.003588622632300269 lambda:12.499701221377165 rho:0.1217708332248737\n",
      "Batch total iterations: 178, parameters: mu:0.004941697090215963 lambda:12.499542136501994 rho:0.16488081266532076\n",
      "Batch total iterations: 126, parameters: mu:0.005449847090021989 lambda:12.49938088680183 rho:0.20619985256632514\n",
      "Batch total iterations: 124, parameters: mu:0.005449847090021989 lambda:12.49938088680183 rho:0.20619985256632514\n",
      "Batch total iterations: 126, parameters: mu:0.005449847090021989 lambda:12.49938088680183 rho:0.20619985256632514\n",
      "Batch total iterations: 91, parameters: mu:0.004943369883487455 lambda:12.499210864886681 rho:0.24633062192369365\n",
      "Batch total iterations: 68, parameters: mu:0.0036456162263087157 lambda:12.499030098168252 rho:0.2851853077393403\n",
      "Batch total iterations: 68, parameters: mu:0.0036456162263087157 lambda:12.499030098168252 rho:0.2851853077393403\n",
      "Batch total iterations: 68, parameters: mu:0.0036456162263087157 lambda:12.499030098168252 rho:0.2851853077393403\n",
      "Batch total iterations: 54, parameters: mu:0.0020613200952881074 lambda:12.49883795869708 rho:0.3226556323938563\n",
      "Batch total iterations: 42, parameters: mu:0.0009448487165590674 lambda:12.498638472841654 rho:0.3583816361044138\n",
      "Batch total iterations: 42, parameters: mu:0.0009448487165590674 lambda:12.498638472841654 rho:0.3583816361044138\n",
      "Batch total iterations: 44, parameters: mu:0.0009448487165590674 lambda:12.498638472841654 rho:0.3583816361044138\n",
      "Batch total iterations: 36, parameters: mu:0.0008967639720696061 lambda:12.498435730059635 rho:0.3921367547806744\n",
      "Batch total iterations: 31, parameters: mu:0.0022349310489360352 lambda:12.4982345662959 rho:0.4237305446429637\n",
      "Batch total iterations: 30, parameters: mu:0.0022349310489360352 lambda:12.4982345662959 rho:0.4237305446429637\n",
      "Batch total iterations: 32, parameters: mu:0.0022349310489360352 lambda:12.4982345662959 rho:0.4237305446429637\n",
      "Batch total iterations: 28, parameters: mu:0.005106529614301488 lambda:12.498038207073973 rho:0.45312629480829925\n",
      "Batch total iterations: 26, parameters: mu:0.009251497842543846 lambda:12.497849962321943 rho:0.48029518305928526\n",
      "Batch total iterations: 26, parameters: mu:0.009251497842543846 lambda:12.497849962321943 rho:0.48029518305928526\n",
      "Epoch 10, Train loss 0.00931498, Validation loss 0.00732108\n",
      "Batch total iterations: 26, parameters: mu:0.009251497842543846 lambda:12.497849962321943 rho:0.48029518305928526\n",
      "Batch total iterations: 27, parameters: mu:0.014515467524432129 lambda:12.497671038458108 rho:0.505334326898032\n",
      "Batch total iterations: 48, parameters: mu:0.02053050897105779 lambda:12.497502793506364 rho:0.5283140331168987\n",
      "Batch total iterations: 46, parameters: mu:0.02053050897105779 lambda:12.497502793506364 rho:0.5283140331168987\n",
      "Batch total iterations: 49, parameters: mu:0.02053050897105779 lambda:12.497502793506364 rho:0.5283140331168987\n",
      "Batch total iterations: 56, parameters: mu:0.026615383836492923 lambda:12.497342427957898 rho:0.5494593338502336\n",
      "Batch total iterations: 61, parameters: mu:0.032334781444756724 lambda:12.497189683609419 rho:0.5688937472312104\n",
      "Batch total iterations: 59, parameters: mu:0.032334781444756724 lambda:12.497189683609419 rho:0.5688937472312104\n",
      "Batch total iterations: 61, parameters: mu:0.032334781444756724 lambda:12.497189683609419 rho:0.5688937472312104\n",
      "Batch total iterations: 60, parameters: mu:0.03752548407541144 lambda:12.49704383118244 rho:0.5867650104312389\n",
      "Batch total iterations: 62, parameters: mu:0.04221945227863777 lambda:12.496905379910988 rho:0.6031651519951134\n",
      "Batch total iterations: 60, parameters: mu:0.04221945227863777 lambda:12.496905379910988 rho:0.6031651519951134\n",
      "Batch total iterations: 62, parameters: mu:0.04221945227863777 lambda:12.496905379910988 rho:0.6031651519951134\n",
      "Batch total iterations: 60, parameters: mu:0.04641711121078703 lambda:12.496773827735975 rho:0.6182201296948733\n",
      "Batch total iterations: 61, parameters: mu:0.050186573062056754 lambda:12.496649775381451 rho:0.6320055637111154\n",
      "Batch total iterations: 60, parameters: mu:0.050186573062056754 lambda:12.496649775381451 rho:0.6320055637111154\n",
      "Batch total iterations: 61, parameters: mu:0.050186573062056754 lambda:12.496649775381451 rho:0.6320055637111154\n",
      "Batch total iterations: 59, parameters: mu:0.05355411521949354 lambda:12.496532785391572 rho:0.6446298021320275\n",
      "Batch total iterations: 59, parameters: mu:0.05658375609569412 lambda:12.496423033143019 rho:0.6561711914387504\n",
      "Batch total iterations: 59, parameters: mu:0.05658375609569412 lambda:12.496423033143019 rho:0.6561711914387504\n",
      "Batch total iterations: 60, parameters: mu:0.05658375609569412 lambda:12.496423033143019 rho:0.6561711914387504\n",
      "Batch total iterations: 58, parameters: mu:0.05930207923571452 lambda:12.496320137846388 rho:0.6667212706645209\n",
      "Batch total iterations: 58, parameters: mu:0.0617536298504378 lambda:12.49622430676449 rho:0.6763438332984422\n",
      "Batch total iterations: 57, parameters: mu:0.0617536298504378 lambda:12.49622430676449 rho:0.6763438332984422\n",
      "Batch total iterations: 58, parameters: mu:0.0617536298504378 lambda:12.49622430676449 rho:0.6763438332984422\n",
      "Batch total iterations: 57, parameters: mu:0.06397241791684698 lambda:12.4961351711171 rho:0.6851177733489096\n",
      "Batch total iterations: 57, parameters: mu:0.06598501150331441 lambda:12.49605271376942 rho:0.6931029636428524\n",
      "Batch total iterations: 56, parameters: mu:0.06598501150331441 lambda:12.49605271376942 rho:0.6931029636428524\n",
      "Batch total iterations: 57, parameters: mu:0.06598501150331441 lambda:12.49605271376942 rho:0.6931029636428524\n",
      "Batch total iterations: 56, parameters: mu:0.06781451673430057 lambda:12.495976572394664 rho:0.7003665204287052\n",
      "Batch total iterations: 56, parameters: mu:0.06949263721419652 lambda:12.495906723197239 rho:0.7069587744314387\n",
      "Batch total iterations: 55, parameters: mu:0.06949263721419652 lambda:12.495906723197239 rho:0.7069587744314387\n",
      "Batch total iterations: 56, parameters: mu:0.06949263721419652 lambda:12.495906723197239 rho:0.7069587744314387\n",
      "Batch total iterations: 55, parameters: mu:0.07103298084709657 lambda:12.495842842015549 rho:0.712935725066834\n",
      "Batch total iterations: 55, parameters: mu:0.0724566058134779 lambda:12.49578469053056 rho:0.7183464244600636\n",
      "Batch total iterations: 54, parameters: mu:0.0724566058134779 lambda:12.49578469053056 rho:0.7183464244600636\n",
      "Batch total iterations: 55, parameters: mu:0.0724566058134779 lambda:12.49578469053056 rho:0.7183464244600636\n",
      "Batch total iterations: 54, parameters: mu:0.07377383692521339 lambda:12.495732037231434 rho:0.7232354297712417\n",
      "Batch total iterations: 54, parameters: mu:0.07500589050220885 lambda:12.495684636089265 rho:0.727645119842241\n",
      "Batch total iterations: 53, parameters: mu:0.07500589050220885 lambda:12.495684636089265 rho:0.727645119842241\n",
      "Epoch 20, Train loss 0.00225034, Validation loss 0.00226772\n",
      "Batch total iterations: 54, parameters: mu:0.07500589050220885 lambda:12.495684636089265 rho:0.727645119842241\n",
      "Batch total iterations: 53, parameters: mu:0.07616384968617976 lambda:12.495642290096852 rho:0.731612552492699\n",
      "Batch total iterations: 53, parameters: mu:0.07725522770255236 lambda:12.49560471568311 rho:0.735174284229553\n",
      "Batch total iterations: 53, parameters: mu:0.07725522770255236 lambda:12.49560471568311 rho:0.735174284229553\n",
      "Batch total iterations: 54, parameters: mu:0.07725522770255236 lambda:12.49560471568311 rho:0.735174284229553\n",
      "Batch total iterations: 52, parameters: mu:0.07828401157531259 lambda:12.495571585321901 rho:0.7383656723458698\n",
      "Batch total iterations: 53, parameters: mu:0.07926738206306855 lambda:12.495542860590012 rho:0.7412112106909923\n",
      "Batch total iterations: 52, parameters: mu:0.07926738206306855 lambda:12.495542860590012 rho:0.7412112106909923\n",
      "Batch total iterations: 53, parameters: mu:0.07926738206306855 lambda:12.495542860590012 rho:0.7412112106909923\n",
      "Batch total iterations: 52, parameters: mu:0.08020141671544997 lambda:12.495518256047557 rho:0.7437397122334145\n",
      "Batch total iterations: 52, parameters: mu:0.0810990525201345 lambda:12.495497354203463 rho:0.7459837195734684\n",
      "Batch total iterations: 52, parameters: mu:0.0810990525201345 lambda:12.495497354203463 rho:0.7459837195734684\n",
      "Batch total iterations: 52, parameters: mu:0.0810990525201345 lambda:12.495497354203463 rho:0.7459837195734684\n",
      "Batch total iterations: 51, parameters: mu:0.08195840363741401 lambda:12.495479969659115 rho:0.7479652925106552\n",
      "Batch total iterations: 52, parameters: mu:0.08279275949788086 lambda:12.495466018004072 rho:0.7497028751401138\n",
      "Batch total iterations: 51, parameters: mu:0.08279275949788086 lambda:12.495466018004072 rho:0.7497028751401138\n",
      "Batch total iterations: 52, parameters: mu:0.08279275949788086 lambda:12.495466018004072 rho:0.7497028751401138\n",
      "Batch total iterations: 51, parameters: mu:0.0836010438751416 lambda:12.495455283794433 rho:0.7512164071001312\n",
      "Batch total iterations: 52, parameters: mu:0.08437746970889597 lambda:12.4954473148056 rho:0.7525318638811358\n",
      "Batch total iterations: 51, parameters: mu:0.08437746970889597 lambda:12.4954473148056 rho:0.7525318638811358\n",
      "Batch total iterations: 52, parameters: mu:0.08437746970889597 lambda:12.4954473148056 rho:0.7525318638811358\n",
      "Batch total iterations: 50, parameters: mu:0.08512304784266711 lambda:12.495441996311701 rho:0.7536636676983997\n",
      "Batch total iterations: 51, parameters: mu:0.08585178984702711 lambda:12.495439197894068 rho:0.7546269814501244\n",
      "Batch total iterations: 51, parameters: mu:0.08585178984702711 lambda:12.495439197894068 rho:0.7546269814501244\n",
      "Batch total iterations: 51, parameters: mu:0.08585178984702711 lambda:12.495439197894068 rho:0.7546269814501244\n",
      "Batch total iterations: 50, parameters: mu:0.0865565752094461 lambda:12.495438618438122 rho:0.7554397825229635\n",
      "Batch total iterations: 51, parameters: mu:0.08724392490882785 lambda:12.495440244526199 rho:0.7561102636317006\n",
      "Batch total iterations: 50, parameters: mu:0.08724392490882785 lambda:12.495440244526199 rho:0.7561102636317006\n",
      "Batch total iterations: 51, parameters: mu:0.08724392490882785 lambda:12.495440244526199 rho:0.7561102636317006\n",
      "Batch total iterations: 50, parameters: mu:0.08791242321706062 lambda:12.495443897252247 rho:0.7566514121708652\n",
      "Batch total iterations: 51, parameters: mu:0.08855763887452944 lambda:12.495449185130171 rho:0.7570815797815912\n",
      "Batch total iterations: 50, parameters: mu:0.08855763887452944 lambda:12.495449185130171 rho:0.7570815797815912\n",
      "Batch total iterations: 50, parameters: mu:0.08855763887452944 lambda:12.495449185130171 rho:0.7570815797815912\n",
      "Batch total iterations: 50, parameters: mu:0.08918575364929905 lambda:12.495456129555143 rho:0.7574063626769507\n",
      "Batch total iterations: 50, parameters: mu:0.08979311368009886 lambda:12.49546447175239 rho:0.757639014020579\n",
      "Batch total iterations: 50, parameters: mu:0.08979311368009886 lambda:12.49546447175239 rho:0.757639014020579\n",
      "Batch total iterations: 50, parameters: mu:0.08979311368009886 lambda:12.49546447175239 rho:0.757639014020579\n",
      "Batch total iterations: 49, parameters: mu:0.09038273200636704 lambda:12.495474122903872 rho:0.7577871673900739\n",
      "Batch total iterations: 50, parameters: mu:0.09095598222759863 lambda:12.495484985378347 rho:0.7578583006962942\n",
      "Batch total iterations: 49, parameters: mu:0.09095598222759863 lambda:12.495484985378347 rho:0.7578583006962942\n",
      "Epoch 30, Train loss 0.00227929, Validation loss 0.00230607\n",
      "Batch total iterations: 50, parameters: mu:0.09095598222759863 lambda:12.495484985378347 rho:0.7578583006962942\n",
      "Batch total iterations: 49, parameters: mu:0.09150965798448774 lambda:12.495496954251921 rho:0.75785930247902\n",
      "Batch total iterations: 50, parameters: mu:0.09204753984341534 lambda:12.49550980195191 rho:0.7578007167686016\n",
      "Batch total iterations: 49, parameters: mu:0.09204753984341534 lambda:12.49550980195191 rho:0.7578007167686016\n",
      "Batch total iterations: 50, parameters: mu:0.09204753984341534 lambda:12.49550980195191 rho:0.7578007167686016\n",
      "Batch total iterations: 49, parameters: mu:0.09257123209711117 lambda:12.495523473783548 rho:0.7576873296623226\n",
      "Batch total iterations: 50, parameters: mu:0.09307211635886951 lambda:12.4955378511175 rho:0.7575255208565649\n",
      "Batch total iterations: 49, parameters: mu:0.09307211635886951 lambda:12.4955378511175 rho:0.7575255208565649\n",
      "Batch total iterations: 50, parameters: mu:0.09307211635886951 lambda:12.4955378511175 rho:0.7575255208565649\n",
      "Batch total iterations: 49, parameters: mu:0.09355296917716273 lambda:12.495552795598137 rho:0.7573219243214556\n",
      "Batch total iterations: 50, parameters: mu:0.0940173060121177 lambda:12.495568312155395 rho:0.7570790847946675\n",
      "Batch total iterations: 49, parameters: mu:0.0940173060121177 lambda:12.495568312155395 rho:0.7570790847946675\n",
      "Batch total iterations: 50, parameters: mu:0.0940173060121177 lambda:12.495568312155395 rho:0.7570790847946675\n",
      "Batch total iterations: 49, parameters: mu:0.09446171839289391 lambda:12.495584261680756 rho:0.7568029058888173\n",
      "Batch total iterations: 49, parameters: mu:0.09488983951687843 lambda:12.495600576583811 rho:0.7564977912132382\n",
      "Batch total iterations: 49, parameters: mu:0.09488983951687843 lambda:12.495600576583811 rho:0.7564977912132382\n",
      "Batch total iterations: 49, parameters: mu:0.09488983951687843 lambda:12.495600576583811 rho:0.7564977912132382\n",
      "Batch total iterations: 49, parameters: mu:0.09530385767533849 lambda:12.495617309499634 rho:0.7561638370557148\n",
      "Batch total iterations: 50, parameters: mu:0.09569970483214792 lambda:12.495634175117798 rho:0.7558112646512192\n",
      "Batch total iterations: 49, parameters: mu:0.09569970483214792 lambda:12.495634175117798 rho:0.7558112646512192\n",
      "Batch total iterations: 49, parameters: mu:0.09569970483214792 lambda:12.495634175117798 rho:0.7558112646512192\n",
      "Batch total iterations: 49, parameters: mu:0.09608209020294689 lambda:12.495651293331926 rho:0.755438091876025\n",
      "Batch total iterations: 49, parameters: mu:0.09644608004598734 lambda:12.495668469946018 rho:0.7550508408644407\n",
      "Batch total iterations: 49, parameters: mu:0.09644608004598734 lambda:12.495668469946018 rho:0.7550508408644407\n",
      "Batch total iterations: 49, parameters: mu:0.09644608004598734 lambda:12.495668469946018 rho:0.7550508408644407\n",
      "Batch total iterations: 49, parameters: mu:0.09679654999149827 lambda:12.495685863241603 rho:0.754646137453951\n",
      "Batch total iterations: 49, parameters: mu:0.09712917593297642 lambda:12.495703131912453 rho:0.7542354047557753\n",
      "Batch total iterations: 48, parameters: mu:0.09712917593297642 lambda:12.495703131912453 rho:0.7542354047557753\n",
      "Batch total iterations: 49, parameters: mu:0.09712917593297642 lambda:12.495703131912453 rho:0.7542354047557753\n",
      "Batch total iterations: 48, parameters: mu:0.09744620960554852 lambda:12.49572032001553 rho:0.7538184545932258\n",
      "Batch total iterations: 49, parameters: mu:0.09775190003573159 lambda:12.495737581357284 rho:0.7533913744753085\n",
      "Batch total iterations: 48, parameters: mu:0.09775190003573159 lambda:12.495737581357284 rho:0.7533913744753085\n",
      "Batch total iterations: 49, parameters: mu:0.09775190003573159 lambda:12.495737581357284 rho:0.7533913744753085\n",
      "Batch total iterations: 49, parameters: mu:0.09804638967150178 lambda:12.495754931730769 rho:0.75295430176573\n",
      "Batch total iterations: 49, parameters: mu:0.09832241706066733 lambda:12.495771972813474 rho:0.7525199015117464\n",
      "Batch total iterations: 48, parameters: mu:0.09832241706066733 lambda:12.495771972813474 rho:0.7525199015117464\n",
      "Batch total iterations: 49, parameters: mu:0.09832241706066733 lambda:12.495771972813474 rho:0.7525199015117464\n",
      "Batch total iterations: 48, parameters: mu:0.0985836000308765 lambda:12.495788831848136 rho:0.7520850857375464\n",
      "Batch total iterations: 49, parameters: mu:0.09883485257496122 lambda:12.495805615468065 rho:0.7516474372960757\n",
      "Batch total iterations: 48, parameters: mu:0.09883485257496122 lambda:12.495805615468065 rho:0.7516474372960757\n",
      "Epoch 40, Train loss 0.00225096, Validation loss 0.00227564\n",
      "Batch total iterations: 49, parameters: mu:0.09883485257496122 lambda:12.495805615468065 rho:0.7516474372960757\n",
      "Batch total iterations: 48, parameters: mu:0.0990706212760211 lambda:12.495822272459241 rho:0.7512084528528719\n",
      "Batch total iterations: 49, parameters: mu:0.09929807882502938 lambda:12.495838691766183 rho:0.7507728161930981\n",
      "Batch total iterations: 48, parameters: mu:0.09929807882502938 lambda:12.495838691766183 rho:0.7507728161930981\n",
      "Batch total iterations: 49, parameters: mu:0.09929807882502938 lambda:12.495838691766183 rho:0.7507728161930981\n",
      "Batch total iterations: 48, parameters: mu:0.09951227743083889 lambda:12.495854841350045 rho:0.7503415107278211\n",
      "Batch total iterations: 49, parameters: mu:0.09971624520811948 lambda:12.49587082280457 rho:0.7499116946419773\n",
      "Batch total iterations: 48, parameters: mu:0.09971624520811948 lambda:12.49587082280457 rho:0.7499116946419773\n",
      "Batch total iterations: 49, parameters: mu:0.09971624520811948 lambda:12.49587082280457 rho:0.7499116946419773\n",
      "Batch total iterations: 48, parameters: mu:0.09990532040798202 lambda:12.495886559603017 rho:0.7494857964757872\n",
      "Batch total iterations: 49, parameters: mu:0.10008715081492246 lambda:12.49590204006417 rho:0.7490651268846782\n",
      "Batch total iterations: 48, parameters: mu:0.10008715081492246 lambda:12.49590204006417 rho:0.7490651268846782\n",
      "Batch total iterations: 49, parameters: mu:0.10008715081492246 lambda:12.49590204006417 rho:0.7490651268846782\n",
      "Batch total iterations: 48, parameters: mu:0.10025828353276046 lambda:12.495917313160623 rho:0.7486478690619065\n",
      "Batch total iterations: 48, parameters: mu:0.1004185798028101 lambda:12.49593224431791 rho:0.748238934958289\n",
      "Batch total iterations: 48, parameters: mu:0.1004185798028101 lambda:12.49593224431791 rho:0.748238934958289\n",
      "Batch total iterations: 49, parameters: mu:0.1004185798028101 lambda:12.49593224431791 rho:0.748238934958289\n",
      "Batch total iterations: 48, parameters: mu:0.10056748490230034 lambda:12.495946882940611 rho:0.7478364935245022\n",
      "Batch total iterations: 48, parameters: mu:0.10070793206815516 lambda:12.495961256008439 rho:0.7474405047741525\n",
      "Batch total iterations: 48, parameters: mu:0.10070793206815516 lambda:12.495961256008439 rho:0.7474405047741525\n",
      "Batch total iterations: 49, parameters: mu:0.10070793206815516 lambda:12.495961256008439 rho:0.7474405047741525\n",
      "Batch total iterations: 48, parameters: mu:0.10083641865986447 lambda:12.4959752689758 rho:0.7470537673953802\n",
      "Batch total iterations: 48, parameters: mu:0.10095846210484768 lambda:12.495989073661825 rho:0.7466717763015768\n",
      "Batch total iterations: 48, parameters: mu:0.10095846210484768 lambda:12.495989073661825 rho:0.7466717763015768\n",
      "Batch total iterations: 49, parameters: mu:0.10095846210484768 lambda:12.495989073661825 rho:0.7466717763015768\n",
      "Batch total iterations: 48, parameters: mu:0.10106863524383027 lambda:12.496002488970207 rho:0.7463001354948895\n",
      "Batch total iterations: 48, parameters: mu:0.10117356670566612 lambda:12.496015722895239 rho:0.7459328829708147\n",
      "Batch total iterations: 48, parameters: mu:0.10117356670566612 lambda:12.496015722895239 rho:0.7459328829708147\n",
      "Batch total iterations: 49, parameters: mu:0.10117356670566612 lambda:12.496015722895239 rho:0.7459328829708147\n",
      "Batch total iterations: 48, parameters: mu:0.10127007777643673 lambda:12.496028639117176 rho:0.745574246413243\n",
      "Batch total iterations: 48, parameters: mu:0.10135891135764255 lambda:12.496041296623059 rho:0.7452224285610837\n",
      "Batch total iterations: 48, parameters: mu:0.10135891135764255 lambda:12.496041296623059 rho:0.7452224285610837\n",
      "Batch total iterations: 49, parameters: mu:0.10135891135764255 lambda:12.496041296623059 rho:0.7452224285610837\n",
      "Batch total iterations: 48, parameters: mu:0.10144203184287352 lambda:12.496053728839753 rho:0.7448766489141027\n",
      "Batch total iterations: 49, parameters: mu:0.10151572038473282 lambda:12.496065806649911 rho:0.7445408076026188\n",
      "Batch total iterations: 48, parameters: mu:0.10151572038473282 lambda:12.496065806649911 rho:0.7445408076026188\n",
      "Batch total iterations: 49, parameters: mu:0.10151572038473282 lambda:12.496065806649911 rho:0.7445408076026188\n",
      "Batch total iterations: 48, parameters: mu:0.10152230024727243 lambda:12.496066994891377 rho:0.7445077137721209\n",
      "Batch total iterations: 49, parameters: mu:0.10152834683626041 lambda:12.496068147889309 rho:0.7444756720257749\n",
      "Batch total iterations: 48, parameters: mu:0.10152834683626041 lambda:12.496068147889309 rho:0.7444756720257749\n",
      "Epoch 50, Train loss 0.00223265, Validation loss 0.00225778\n",
      "Batch total iterations: 49, parameters: mu:0.10152834683626041 lambda:12.496068147889309 rho:0.7444756720257749\n",
      "Batch total iterations: 48, parameters: mu:0.10153371774614467 lambda:12.496069270440218 rho:0.7444445295439458\n",
      "Batch total iterations: 48, parameters: mu:0.10153869136804498 lambda:12.496070380060836 rho:0.7444137005491427\n",
      "Batch total iterations: 48, parameters: mu:0.10153869136804498 lambda:12.496070380060836 rho:0.7444137005491427\n",
      "Batch total iterations: 49, parameters: mu:0.10153869136804498 lambda:12.496070380060836 rho:0.7444137005491427\n",
      "Batch total iterations: 48, parameters: mu:0.10154304437756606 lambda:12.496071471595762 rho:0.7443833608592094\n",
      "Batch total iterations: 48, parameters: mu:0.10154711751939827 lambda:12.496072543951863 rho:0.7443535841324562\n",
      "Batch total iterations: 48, parameters: mu:0.10154711751939827 lambda:12.496072543951863 rho:0.7443535841324562\n",
      "Batch total iterations: 49, parameters: mu:0.10154711751939827 lambda:12.496072543951863 rho:0.7443535841324562\n",
      "Batch total iterations: 48, parameters: mu:0.10155058357283737 lambda:12.49607359737941 rho:0.7443243376345965\n",
      "Batch total iterations: 48, parameters: mu:0.10155390021958095 lambda:12.496074638589269 rho:0.7442954417991675\n",
      "Batch total iterations: 48, parameters: mu:0.10155390021958095 lambda:12.496074638589269 rho:0.7442954417991675\n",
      "Batch total iterations: 49, parameters: mu:0.10155390021958095 lambda:12.496074638589269 rho:0.7442954417991675\n",
      "Batch total iterations: 48, parameters: mu:0.10155638285134548 lambda:12.496075651306592 rho:0.7442673775399168\n",
      "Batch total iterations: 48, parameters: mu:0.10155913480703428 lambda:12.496076668184491 rho:0.744239170318071\n",
      "Batch total iterations: 48, parameters: mu:0.10155913480703428 lambda:12.496076668184491 rho:0.744239170318071\n",
      "Batch total iterations: 49, parameters: mu:0.10155913480703428 lambda:12.496076668184491 rho:0.744239170318071\n",
      "Batch total iterations: 48, parameters: mu:0.10156135128783578 lambda:12.496077662828737 rho:0.7442116161758464\n",
      "Batch total iterations: 48, parameters: mu:0.10156364224165784 lambda:12.496078659764716 rho:0.7441839758966925\n",
      "Batch total iterations: 48, parameters: mu:0.10156364224165784 lambda:12.496078659764716 rho:0.7441839758966925\n",
      "Batch total iterations: 49, parameters: mu:0.10156364224165784 lambda:12.496078659764716 rho:0.7441839758966925\n",
      "Batch total iterations: 48, parameters: mu:0.10156540825706942 lambda:12.496079651158034 rho:0.7441564619221771\n",
      "Batch total iterations: 48, parameters: mu:0.10156729133635566 lambda:12.496080628825455 rho:0.744129375998229\n",
      "Batch total iterations: 48, parameters: mu:0.10156729133635566 lambda:12.496080628825455 rho:0.744129375998229\n",
      "Batch total iterations: 49, parameters: mu:0.10156729133635566 lambda:12.496080628825455 rho:0.744129375998229\n",
      "Batch total iterations: 48, parameters: mu:0.10156878580509608 lambda:12.496081605146738 rho:0.7441022748927459\n",
      "Batch total iterations: 49, parameters: mu:0.10157031265948363 lambda:12.496082566428901 rho:0.7440756646010783\n",
      "Batch total iterations: 48, parameters: mu:0.10157031265948363 lambda:12.496082566428901 rho:0.7440756646010783\n",
      "Batch total iterations: 49, parameters: mu:0.10157031265948363 lambda:12.496082566428901 rho:0.7440756646010783\n",
      "Batch total iterations: 48, parameters: mu:0.10157168464169757 lambda:12.496083509346938 rho:0.7440496319935397\n",
      "Batch total iterations: 48, parameters: mu:0.10157285675089099 lambda:12.496084457506575 rho:0.7440233953724527\n",
      "Batch total iterations: 48, parameters: mu:0.10157285675089099 lambda:12.496084457506575 rho:0.7440233953724527\n",
      "Batch total iterations: 49, parameters: mu:0.10157285675089099 lambda:12.496084457506575 rho:0.7440233953724527\n",
      "Batch total iterations: 48, parameters: mu:0.10157407231790515 lambda:12.496085407088385 rho:0.7439970952694643\n",
      "Batch total iterations: 49, parameters: mu:0.10157497064947554 lambda:12.496086343026095 rho:0.7439712159581815\n",
      "Batch total iterations: 48, parameters: mu:0.10157497064947554 lambda:12.496086343026095 rho:0.7439712159581815\n",
      "Batch total iterations: 38, parameters: mu:0.10157497064947554 lambda:12.496086343026095 rho:0.7439712159581815\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "plot_admm_vs_admm_1d_reconstruction() missing 1 required positional argument: 'epochs'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-7-bbc178993a73>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     15\u001B[0m \u001B[0ms_hat_admm\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mvanilla_admm\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mb_x\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mH\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mH\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmax_itr\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mmax_iter\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     16\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 17\u001B[0;31m plot_admm_vs_admm_1d_reconstruction(s_hat_admm=s_hat_admm,\n\u001B[0m\u001B[1;32m     18\u001B[0m                                   s_hat_ladmm=s_hat_ladmm, max_iter=max_iter,s_gt = s_gt)\n\u001B[1;32m     19\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mTypeError\u001B[0m: plot_admm_vs_admm_1d_reconstruction() missing 1 required positional argument: 'epochs'"
     ]
    }
   ],
   "source": [
    "max_iter, epochs = 1000, 60\n",
    "\n",
    "\n",
    "# b_x, b_s = a batch from the validation set\n",
    "# Train and apply L-ADMM One Parameter with T iterations / layers\n",
    "ladmm_mse, admm1d_model, b_x, b_s = admm_1d_apply(train_loader, test_loader, max_iter, H)\n",
    "\n",
    "admm_mse = admm_apply(test_loader, max_iter, H)\n",
    "\n",
    "######################### Visuallization #########################\n",
    "b_x, s_gt = b_x[0], b_s[0]\n",
    "s_hat_ladmm = admm1d_model(b_x)\n",
    "s_hat_ladmm = s_hat_ladmm.detach().numpy()[0]\n",
    "\n",
    "s_hat_admm = vanilla_admm(x=b_x, H=H, max_itr=max_iter)\n",
    "\n",
    "plot_admm_vs_admm_1d_reconstruction(s_hat_admm=s_hat_admm,\n",
    "                                  s_hat_ladmm=s_hat_ladmm, max_iter=max_iter,s_gt = s_gt)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_admm_vs_admm_1d_reconstruction(s_hat_admm=s_hat_admm,\n",
    "                                    s_hat_ladmm=s_hat_ladmm,\n",
    "                                    max_iter=max_iter, s_gt = s_gt, epochs=epochs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
