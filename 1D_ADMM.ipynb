{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "import torch, numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as Data\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "SEED = 0\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "torch.set_default_dtype(torch.float64)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data\n",
    "Next, we generate a data set $\\mathcal{D}=\\{(\\boldsymbol{s}_t, \\boldsymbol{x}_t)\\}_{t=1}^{n_t}$, where each $\\boldsymbol{s}_t$ has $m=200$ entries, out of which at most $k=4$ are non-zero, while $\\boldsymbol{x}_t$ has $n=150$ entires and is obtained via\n",
    "\\begin{equation}\n",
    "\\boldsymbol{x}_t = \\boldsymbol{H}\\boldsymbol{s}_t + \\boldsymbol{w}_t,\n",
    "\\end{equation}\n",
    "with $\\boldsymbol{w}_t$ being i.i.d. Gaussian noise.\n",
    "\n",
    "To that aim, we generate a dedicated class inheriting Dataset to get the data samples, and to allow each tuple to be comprised of both $(\\boldsymbol{s}_t, \\boldsymbol{x}_t)$ as well as $\\boldsymbol{H}$."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "class SimulatedData(Data.Dataset):\n",
    "    def __init__(self, x, H, s):\n",
    "        self.x = x\n",
    "        self.s = s\n",
    "        self.H = H\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.x.shape[1]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.x[:, idx]\n",
    "        H = self.H\n",
    "        s = self.s[:, idx]\n",
    "        return x, H, s"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The next function creates a dataset.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "def create_data_set(H, n, m, k, N=1000, batch_size=512, snr=30, noise_dev=0.5):\n",
    "    # Initialization\n",
    "\n",
    "    x = torch.zeros(n, N)\n",
    "    s = torch.zeros(m, N)\n",
    "    # Create signals\n",
    "    for i in range(N):\n",
    "        # Create a sparsed signal s\n",
    "        index_k = np.random.choice(m, k, replace=False)\n",
    "        peaks = noise_dev * np.random.randn(k)\n",
    "\n",
    "        s[index_k, i] = torch.from_numpy(peaks).to(s)\n",
    "\n",
    "        # X = Hs+w\n",
    "        x[:, i] = H @ s[:, i] + 0.01 * np.random.randn(n)\n",
    "\n",
    "    simulated = SimulatedData(x=x, H=H, s=s)\n",
    "    data_loader = Data.DataLoader(dataset=simulated, batch_size=batch_size, shuffle=True)\n",
    "    return data_loader"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, we generate the data set\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "n, m, k = 150, 200, 4\n",
    "\n",
    "# Measurement matrix\n",
    "H = torch.randn(n, m)\n",
    "H /= torch.norm(H, dim=0)\n",
    "\n",
    "train_loader = create_data_set(H, n=n, m=m, k=k, N=1000)\n",
    "\n",
    "test_loader = create_data_set(H, n=n, m=m, k=k, N=1000)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Vanilla ADMM implementation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vanilla_admm(x, H, lambda_=12.5, mu=0.00005, rho=0.01, max_itr=300, eps=10 ** -5):\n",
    "    proj = torch.nn.Softshrink(rho / (2 * lambda_))\n",
    "\n",
    "    # initial estimate\n",
    "    s = torch.zeros((H.shape[1]))\n",
    "    u = torch.zeros((H.shape[1]))\n",
    "    v = torch.zeros((H.shape[1]))\n",
    "\n",
    "    # left_term = (H^TH+2λI)^-1 2*lambda or rho?\n",
    "    left_term = torch.linalg.inv(H.T @ H + rho * torch.eye(H.shape[1]))\n",
    "\n",
    "    recovery_errors = []\n",
    "    for k in range(max_itr):\n",
    "        s_prev, v_prev, u_prev = s, v, u\n",
    "\n",
    "        # Update s_k+1 = ((H^T)H+2λI)^−1(H^T x+2λ(vk−uk)).\n",
    "        right_term = H.T @ x + rho * (v_prev - u_prev)\n",
    "        s = left_term @ right_term\n",
    "\n",
    "        # Update vk+1 = prox_(1/2λϕ)(sk+1 + uk)\n",
    "        v = proj(s + u_prev)\n",
    "\n",
    "        # Update uk+1 = uk + μ (sk+1 − vk+1).\n",
    "        u = u_prev + mu * (s - v)\n",
    "\n",
    "        # cease if convergence achieved\n",
    "        if torch.sum(torch.abs(s - s_prev)) <= eps:\n",
    "            break\n",
    "\n",
    "        # save recovery error\n",
    "        recovery_errors.append(torch.sum((torch.matmul(H, s) - x) ** 2))\n",
    "\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "1D Model Based ADMM implementation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "EPSILON = 10 ** -2\n",
    "\n",
    "class LADMM_Model_1D(nn.Module):\n",
    "    def __init__(self, n, m, max_iterations=1000, rho=0.01, H=None, lambda_=12.5, mu=0.00005, epsilon=EPSILON):\n",
    "        super(LADMM_Model_1D, self).__init__()\n",
    "        self.n, self.m = n, m\n",
    "        self.H = H\n",
    "\n",
    "        # admm(x, H, lambda_=12.5, mu=0.00005, rho=0.01, max_itr=300, eps=10 ** -5):\n",
    "\n",
    "        # Initialization of 1 dimensional parameter\n",
    "        self.rho = nn.Parameter(torch.ones(1) * rho, requires_grad=True)\n",
    "        self.lambda_ = nn.Parameter(torch.ones(1) * lambda_, requires_grad=True)\n",
    "        self.mu = nn.Parameter(torch.ones(1) * mu, requires_grad=True)\n",
    "\n",
    "        self.max_iteration = max_iterations\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def _shrink(self, s, beta, rho):\n",
    "        return beta * F.softshrink(s / beta, lambd=rho)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x:\n",
    "        Returns:\n",
    "        \"\"\"\n",
    "\n",
    "        s_prev = torch.zeros(x.shape[0], self.H.shape[1])\n",
    "        u_prev = torch.zeros((x.shape[0], self.H.shape[1]))\n",
    "        v_prev = torch.zeros((x.shape[0], self.H.shape[1]))\n",
    "\n",
    "        #################### Iteration 0 ####################\n",
    "\n",
    "        left_term = torch.linalg.inv(H.T @ H + self.rho * torch.eye(H.shape[1]))\n",
    "\n",
    "        right_term = (H.T @ x.T).T + self.rho * (v_prev - u_prev)\n",
    "\n",
    "        s = (left_term @ right_term.T).T\n",
    "        v = self._shrink(s + u_prev, self.rho / (2 * self.lambda_), rho=self.rho.item())\n",
    "        u = u_prev + self.mu * (s - v)\n",
    "\n",
    "        ######################################################\n",
    "        iteration = 0\n",
    "\n",
    "        # Notice the stopping condition isn't fixed K iterations\n",
    "        while (torch.norm(s_prev.detach() - s.detach()).item() > self.epsilon) and (iteration < self.max_iteration):\n",
    "            s_prev, v_prev, u_prev = s, v, u\n",
    "\n",
    "            # left_term = (H^TH+2λI)^-1 2*lambda or rho?\n",
    "            left_term = torch.linalg.inv(H.T @ H + self.rho * torch.eye(H.shape[1]))\n",
    "\n",
    "            right_term = (H.T @ x.T).T + self.rho * (v_prev - u_prev)\n",
    "\n",
    "            # Update s_k+1 = ((H^T)H+2λI)^−1(H^T x+2λ(vk−uk)).\n",
    "            s = (left_term @ right_term.T).T\n",
    "\n",
    "            # Update vk+1 = prox_(1/2λϕ)(sk+1 + uk)\n",
    "            v = self._shrink(s + u_prev, self.rho / (2 * self.lambda_), rho=self.rho.item())\n",
    "\n",
    "            # Update uk+1 = uk + μ (sk+1 − vk+1).\n",
    "            u = u_prev + self.mu * (s - v)\n",
    "            iteration += 1\n",
    "\n",
    "        # print(\"Batch total iterations: {0}, parameters: mu:{1} lambda:{2} rho:{3}\".format(iteration, self.mu.item(),\n",
    "        #                                                                                   self.lambda_.item(),\n",
    "        #                                                                                   self.rho.item()))\n",
    "        return s"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train and Validation\n",
    "\n",
    "For training, we use SGDM with learning rate scheduler and the $\\ell_2$ loss"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "def train(model, train_loader, valid_loader, num_epochs=60):\n",
    "    \"\"\"Train a network.\n",
    "    Returns:\n",
    "        loss_test {numpy} -- loss function values on test set\n",
    "    \"\"\"\n",
    "    # Initialization\n",
    "    optimizer = torch.optim.SGD(\n",
    "        model.parameters(),\n",
    "        lr=5e-05,\n",
    "        momentum=0.9,\n",
    "        weight_decay=0,\n",
    "    )\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.1)\n",
    "    loss_train = np.zeros((num_epochs,))\n",
    "    loss_test = np.zeros((num_epochs,))\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for step, (b_x, b_H, b_s) in enumerate(train_loader):\n",
    "\n",
    "            s_hat = model(b_x)\n",
    "            loss = F.mse_loss(s_hat, b_s, reduction=\"sum\")\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            model.zero_grad()\n",
    "            train_loss += loss.data.item()\n",
    "\n",
    "        # Aggregate loss\n",
    "        loss_train[epoch] = train_loss / len(train_loader.dataset)\n",
    "        scheduler.step()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        for step, (b_x, b_H, b_s) in enumerate(valid_loader):\n",
    "            s_hat = model(b_x)\n",
    "            test_loss += F.mse_loss(s_hat, b_s, reduction=\"sum\").data.item()\n",
    "        loss_test[epoch] = test_loss / len(valid_loader.dataset)\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(\"Epoch %d, Train loss %.8f, Validation loss %.8f\" % (epoch, loss_train[epoch], loss_test[epoch]))\n",
    "\n",
    "    return loss_test, b_x, b_s"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following functions apply 1D L-ADMM and ADMM for given data sets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "def admm_1d_apply(train_loader, test_loader, max_iterations, H):\n",
    "    n = H.shape[1]\n",
    "    m = H.shape[1]\n",
    "\n",
    "    ladmm = LADMM_Model_1D(n=n, m=m, max_iterations=max_iterations, H=H)\n",
    "\n",
    "    loss_test, b_x, b_s = train(ladmm, train_loader, test_loader)\n",
    "    error = loss_test[-1]\n",
    "\n",
    "    return error, ladmm, b_x, b_s\n",
    "\n",
    "\n",
    "def admm_apply(test_loader, T, H):\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for step, (x, _, s) in enumerate(test_loader.dataset):\n",
    "        s_hat = vanilla_admm(x=x, H=H, max_itr=T)\n",
    "        loss += F.mse_loss(s_hat, s, reduction=\"sum\").data.item()\n",
    "\n",
    "    return loss / len(test_loader.dataset)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plotting utilities for displaying the results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "def plot_admm_vs_admm_1d_reconstruction(s_hat_ladmm, s_hat_admm, max_iter, s_gt, epochs):\n",
    "    plt.figure()\n",
    "    plt.subplot(2, 1, 1)\n",
    "    # plt.title(\"L-ADMM K={0}\".format(k_l_admm.T), fontsize=10)\n",
    "    plt.plot(s_hat_ladmm, '.--', label='One-Parameter-ADMM, epochs={0}, #EpochMaxIter {1}'.format(epochs, max_iter),\n",
    "             color='r', linewidth=1)\n",
    "    plt.plot(s_gt, label='sparse signal', color='k')\n",
    "    plt.xlabel('Index', fontsize=10)\n",
    "    plt.ylabel('Value', fontsize=10)\n",
    "    plt.legend()\n",
    "\n",
    "    axs1 = plt.subplot(2, 1, 2)\n",
    "\n",
    "    new_pos = axs1.get_position()\n",
    "    new_pos.y0 -= 0.15 * new_pos.y0\n",
    "    new_pos.y1 -= 0.15 * new_pos.y1\n",
    "    axs1.set_position(pos=new_pos)\n",
    "\n",
    "    plt.plot(s_hat_admm, '.--', label='ADMM, MaxIter={0}'.format(max_iter), color='r', linewidth=1)\n",
    "    plt.plot(s_gt, label='sparse signal', color='k')\n",
    "    plt.xlabel('Index', fontsize=10)\n",
    "    plt.ylabel('Value', fontsize=10)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Train loss 0.29971676, Validation loss 0.29008598\n",
      "Epoch 10, Train loss 0.01025452, Validation loss 0.00792984\n",
      "Epoch 20, Train loss 0.00230744, Validation loss 0.00228909\n",
      "Epoch 30, Train loss 0.00233179, Validation loss 0.00232318\n",
      "Epoch 40, Train loss 0.00230617, Validation loss 0.00229499\n",
      "Epoch 50, Train loss 0.00228866, Validation loss 0.00227687\n"
     ]
    }
   ],
   "source": [
    "max_iter, epochs = 1000, 60\n",
    "\n",
    "# Train and apply L-ADMM One Parameter with T iterations / layers. b_x, b_s are a batch from the validation set\n",
    "ladmm_mse, admm1d_model, b_x, b_s = admm_1d_apply(train_loader, test_loader, max_iter, H)\n",
    "\n",
    "admm_mse = admm_apply(test_loader, max_iter, H)\n",
    "\n",
    "\n",
    "b_x, s_gt = b_x[0], b_s[0]\n",
    "s_hat_ladmm = admm1d_model(b_x)\n",
    "s_hat_ladmm = s_hat_ladmm.detach().numpy()[0]\n",
    "\n",
    "s_hat_admm = vanilla_admm(x=b_x, H=H, max_itr=max_iter)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_admm_vs_admm_1d_reconstruction(s_hat_admm=s_hat_admm,\n",
    "                                    s_hat_ladmm=s_hat_ladmm,\n",
    "                                    max_iter=max_iter, s_gt = s_gt, epochs=epochs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
