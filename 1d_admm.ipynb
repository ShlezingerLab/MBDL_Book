{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "import torch, numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as Data\n",
    "\n",
    "\n",
    "from utills import plot_admm_vs_admm_1d_reconstruction\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "torch.set_default_dtype(torch.float64)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data\n",
    "Next, we generate a data set $\\mathcal{D}=\\{(\\boldsymbol{s}_t, \\boldsymbol{x}_t)\\}_{t=1}^{n_t}$, where each $\\boldsymbol{s}_t$ has $m=200$ entries, out of which at most $k=4$ are non-zero, while $\\boldsymbol{x}_t$ has $n=150$ entires and is obtained via\n",
    "\\begin{equation}\n",
    "\\boldsymbol{x}_t = \\boldsymbol{H}\\boldsymbol{s}_t + \\boldsymbol{w}_t,\n",
    "\\end{equation}\n",
    "with $\\boldsymbol{w}_t$ being i.i.d. Gaussian noise.\n",
    "\n",
    "To that aim, we generate a dedicated class inheriting Dataset to get the data samples, and to allow each tuple to be comprised of both $(\\boldsymbol{s}_t, \\boldsymbol{x}_t)$ as well as $\\boldsymbol{H}$."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "class SimulatedData(Data.Dataset):\n",
    "    def __init__(self, x, H, s):\n",
    "        self.x = x\n",
    "        self.s = s\n",
    "        self.H = H\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.x.shape[1]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.x[:, idx]\n",
    "        H = self.H\n",
    "        s = self.s[:, idx]\n",
    "        return x, H, s"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The next function creates a dataset.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def create_data_set(H, n, m, k, N=1000, batch_size=512, snr=30, noise_dev=0.5):\n",
    "    # Initialization\n",
    "\n",
    "    x = torch.zeros(n, N)\n",
    "    s = torch.zeros(m, N)\n",
    "    # Create signals\n",
    "    for i in range(N):\n",
    "        # Create a sparsed signal s\n",
    "        index_k = np.random.choice(m, k, replace=False)\n",
    "        peaks = noise_dev * np.random.randn(k)\n",
    "\n",
    "        s[index_k, i] = torch.from_numpy(peaks).to(s)\n",
    "\n",
    "        # X = Hs+w\n",
    "        x[:, i] = H @ s[:, i] + 0.01 * np.random.randn(n)\n",
    "\n",
    "    simulated = SimulatedData(x=x, H=H, s=s)\n",
    "    data_loader = Data.DataLoader(dataset=simulated, batch_size=batch_size, shuffle=True)\n",
    "    return data_loader"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, we generate the data set\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "n, m, k = 150, 200, 4\n",
    "\n",
    "# Measurement matrix\n",
    "H = torch.randn(n, m)\n",
    "H /= torch.norm(H, dim=0)\n",
    "\n",
    "train_loader = create_data_set(H, n=n, m=m, k=k, N=1000)\n",
    "\n",
    "test_loader = create_data_set(H, n=n, m=m, k=k, N=1000)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Vanilla ADMM implementation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vanilla_admm(x, H, lambda_=12.5, mu=0.00005, rho=0.01, max_itr=300, eps=10 ** -5):\n",
    "    proj = torch.nn.Softshrink(rho / (2 * lambda_))\n",
    "\n",
    "    # initial estimate\n",
    "    s = torch.zeros((H.shape[1]))\n",
    "    u = torch.zeros((H.shape[1]))\n",
    "    v = torch.zeros((H.shape[1]))\n",
    "\n",
    "    # left_term = (H^TH+2λI)^-1 2*lambda or rho?\n",
    "    left_term = torch.linalg.inv(H.T @ H + rho * torch.eye(H.shape[1]))\n",
    "\n",
    "    recovery_errors = []\n",
    "    for k in range(max_itr):\n",
    "        s_prev, v_prev, u_prev = s, v, u\n",
    "\n",
    "        # Update s_k+1 = ((H^T)H+2λI)^−1(H^T x+2λ(vk−uk)).\n",
    "\n",
    "        right_term = H.T @ x + rho * (v_prev - u_prev)\n",
    "        s = left_term @ right_term\n",
    "\n",
    "        # Update vk+1 = prox_(1/2λϕ)(sk+1 + uk)\n",
    "        v = proj(s + u_prev)\n",
    "\n",
    "        # Update uk+1 = uk + μ (sk+1 − vk+1).\n",
    "\n",
    "        u = u_prev + mu * (s - v)\n",
    "\n",
    "        # # cease if convergence achieved\n",
    "        if torch.sum(torch.abs(s - s_prev)) <= eps:\n",
    "            break\n",
    "\n",
    "        # save recovery error\n",
    "        recovery_errors.append(torch.sum((torch.matmul(H, s) - x) ** 2))\n",
    "\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "1D Model Based ADMM implementation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "EPSILON = 10 ** -2\n",
    "\n",
    "class LADMM_Model_1D(nn.Module):\n",
    "    def __init__(self, n, m, max_iterations=1000, rho=0.01, H=None, lambda_=12.5, mu=0.00005, epsilon=EPSILON):\n",
    "        super(LADMM_Model_1D, self).__init__()\n",
    "        self.n, self.m = n, m\n",
    "        self.H = H\n",
    "\n",
    "        # admm(x, H, lambda_=12.5, mu=0.00005, rho=0.01, max_itr=300, eps=10 ** -5):\n",
    "\n",
    "        # Initialization of 1 dimensional parameter\n",
    "        self.rho = nn.Parameter(torch.ones(1) * rho, requires_grad=True)\n",
    "        self.lambda_ = nn.Parameter(torch.ones(1) * lambda_, requires_grad=True)\n",
    "        self.mu = nn.Parameter(torch.ones(1) * mu, requires_grad=True)\n",
    "\n",
    "        self.max_iteration = max_iterations\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def _shrink(self, s, beta, rho):\n",
    "        return beta * F.softshrink(s / beta, lambd=rho)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x:\n",
    "        Returns:\n",
    "        \"\"\"\n",
    "\n",
    "        s_prev = torch.zeros(x.shape[0], self.H.shape[1])\n",
    "        u_prev = torch.zeros((x.shape[0], self.H.shape[1]))\n",
    "        v_prev = torch.zeros((x.shape[0], self.H.shape[1]))\n",
    "\n",
    "        #################### Iteration 0 ####################\n",
    "\n",
    "        left_term = torch.linalg.inv(H.T @ H + self.rho * torch.eye(H.shape[1]))\n",
    "\n",
    "        right_term = (H.T @ x.T).T + self.rho * (v_prev - u_prev)\n",
    "\n",
    "        s = (left_term @ right_term.T).T\n",
    "        v = self._shrink(s + u_prev, self.rho / (2 * self.lambda_), rho=self.rho.item())\n",
    "        u = u_prev + self.mu * (s - v)\n",
    "\n",
    "        ######################################################\n",
    "        iteration = 0\n",
    "\n",
    "        # Notice the stopping condition isn't fixed K iterations\n",
    "        while (torch.norm(s_prev.detach() - s.detach()).item() > self.epsilon) and (iteration < self.max_iteration):\n",
    "            s_prev, v_prev, u_prev = s, v, u\n",
    "\n",
    "            # left_term = (H^TH+2λI)^-1 2*lambda or rho?\n",
    "            left_term = torch.linalg.inv(H.T @ H + self.rho * torch.eye(H.shape[1]))\n",
    "\n",
    "            right_term = (H.T @ x.T).T + self.rho * (v_prev - u_prev)\n",
    "\n",
    "            # Update s_k+1 = ((H^T)H+2λI)^−1(H^T x+2λ(vk−uk)).\n",
    "            s = (left_term @ right_term.T).T\n",
    "\n",
    "            # Update vk+1 = prox_(1/2λϕ)(sk+1 + uk)\n",
    "            v = self._shrink(s + u_prev, self.rho / (2 * self.lambda_), rho=self.rho.item())\n",
    "\n",
    "            # Update uk+1 = uk + μ (sk+1 − vk+1).\n",
    "            u = u_prev + self.mu * (s - v)\n",
    "            iteration += 1\n",
    "\n",
    "        print(\"Batch total iterations: {0}, parameters: mu:{1} lambda:{2} rho:{3}\".format(iteration, self.mu.item(),\n",
    "                                                                                          self.lambda_.item(),\n",
    "                                                                                          self.rho.item()))\n",
    "        return s"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train and Validation\n",
    "\n",
    "For training, we use SGDM with learning rate scheduler and the $\\ell_2$ loss"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "def train(model, train_loader, valid_loader, num_epochs=60):\n",
    "    \"\"\"Train a network.\n",
    "    Returns:\n",
    "        loss_test {numpy} -- loss function values on test set\n",
    "    \"\"\"\n",
    "    # Initialization\n",
    "    optimizer = torch.optim.SGD(\n",
    "        model.parameters(),\n",
    "        lr=5e-05,\n",
    "        momentum=0.9,\n",
    "        weight_decay=0,\n",
    "    )\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.1)\n",
    "    loss_train = np.zeros((num_epochs,))\n",
    "    loss_test = np.zeros((num_epochs,))\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for step, (b_x, b_H, b_s) in enumerate(train_loader):\n",
    "\n",
    "            s_hat = model(b_x)\n",
    "            loss = F.mse_loss(s_hat, b_s, reduction=\"sum\")\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            model.zero_grad()\n",
    "            train_loss += loss.data.item()\n",
    "\n",
    "        # Aggregate loss\n",
    "        loss_train[epoch] = train_loss / len(train_loader.dataset)\n",
    "        scheduler.step()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        for step, (b_x, b_H, b_s) in enumerate(valid_loader):\n",
    "            s_hat = model(b_x)\n",
    "            test_loss += F.mse_loss(s_hat, b_s, reduction=\"sum\").data.item()\n",
    "        loss_test[epoch] = test_loss / len(valid_loader.dataset)\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(\"Epoch %d, Train loss %.8f, Validation loss %.8f\" % (epoch, loss_train[epoch], loss_test[epoch]))\n",
    "\n",
    "    return loss_test, b_x, b_s"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following functions apply 1D L-ADMM and ADMM for given data sets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "def admm_1d_apply(train_loader, test_loader, max_iterations, H):\n",
    "    n = H.shape[1]\n",
    "    m = H.shape[1]\n",
    "\n",
    "    ladmm = LADMM_Model_1D(n=n, m=m, max_iterations=max_iterations, H=H)\n",
    "\n",
    "    loss_test, b_x, b_s = train(ladmm, train_loader, test_loader)\n",
    "    error = loss_test[-1]\n",
    "\n",
    "    return error, ladmm, b_x, b_s\n",
    "\n",
    "\n",
    "def admm_apply(test_loader, T, H):\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for step, (x, _, s) in enumerate(test_loader.dataset):\n",
    "        s_hat = vanilla_admm(x=x, H=H, max_itr=T)\n",
    "        loss += F.mse_loss(s_hat, s, reduction=\"sum\").data.item()\n",
    "\n",
    "    return loss / len(test_loader.dataset)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch total iterations: 4, parameters: mu:5e-05 lambda:12.5 rho:0.01\n",
      "Batch total iterations: 4, parameters: mu:5.303270573904955e-05 lambda:12.49999975733513 rho:0.010418083858229708\n",
      "Batch total iterations: 4, parameters: mu:5.886895383521976e-05 lambda:12.499999290345022 rho:0.011215745383113106\n",
      "Batch total iterations: 5, parameters: mu:5.886895383521976e-05 lambda:12.499999290345022 rho:0.011215745383113106\n",
      "Epoch 0, Train loss 0.29968630, Validation loss 0.29134754\n",
      "Batch total iterations: 4, parameters: mu:5.886895383521976e-05 lambda:12.499999290345022 rho:0.011215745383113106\n",
      "Batch total iterations: 5, parameters: mu:6.785587832789199e-05 lambda:12.499998571244793 rho:0.012417740044253648\n",
      "Batch total iterations: 5, parameters: mu:8.158652968964355e-05 lambda:12.499997472521779 rho:0.014018384810006201\n",
      "Batch total iterations: 5, parameters: mu:8.158652968964355e-05 lambda:12.499997472521779 rho:0.014018384810006201\n",
      "Batch total iterations: 5, parameters: mu:8.158652968964355e-05 lambda:12.499997472521779 rho:0.014018384810006201\n",
      "Batch total iterations: 5, parameters: mu:0.00010113359144333721 lambda:12.499995908342717 rho:0.016073655373186686\n",
      "Batch total iterations: 6, parameters: mu:0.00012833237352724303 lambda:12.499993731740258 rho:0.0186476550627192\n",
      "Batch total iterations: 6, parameters: mu:0.00012833237352724303 lambda:12.499993731740258 rho:0.0186476550627192\n",
      "Batch total iterations: 6, parameters: mu:0.00012833237352724303 lambda:12.499993731740258 rho:0.0186476550627192\n",
      "Batch total iterations: 6, parameters: mu:0.00016864908446013693 lambda:12.499990505032768 rho:0.021901253520130837\n",
      "Batch total iterations: 7, parameters: mu:0.0002255466458467553 lambda:12.499985950712706 rho:0.02591372015369714\n",
      "Batch total iterations: 7, parameters: mu:0.0002255466458467553 lambda:12.499985950712706 rho:0.02591372015369714\n",
      "Batch total iterations: 7, parameters: mu:0.0002255466458467553 lambda:12.499985950712706 rho:0.02591372015369714\n",
      "Batch total iterations: 8, parameters: mu:0.0003114130492342074 lambda:12.499979075945971 rho:0.030988536276862967\n",
      "Batch total iterations: 102, parameters: mu:0.00044508244073528764 lambda:12.499968368135134 rho:0.03747760559450476\n",
      "Batch total iterations: 91, parameters: mu:0.00044508244073528764 lambda:12.499968368135134 rho:0.03747760559450476\n",
      "Batch total iterations: 118, parameters: mu:0.00044508244073528764 lambda:12.499968368135134 rho:0.03747760559450476\n",
      "Batch total iterations: 456, parameters: mu:0.0015685961108392208 lambda:12.499876254595467 rho:0.07088727274633436\n",
      "Batch total iterations: 279, parameters: mu:0.0034027207357706438 lambda:12.499719914083252 rho:0.11420830579302134\n",
      "Batch total iterations: 284, parameters: mu:0.0034027207357706438 lambda:12.499719914083252 rho:0.11420830579302134\n",
      "Batch total iterations: 293, parameters: mu:0.0034027207357706438 lambda:12.499719914083252 rho:0.11420830579302134\n",
      "Batch total iterations: 187, parameters: mu:0.004825612592311819 lambda:12.499558127614884 rho:0.15621692657690503\n",
      "Batch total iterations: 134, parameters: mu:0.005462370149052378 lambda:12.499395786804916 rho:0.19644613298109947\n",
      "Batch total iterations: 136, parameters: mu:0.005462370149052378 lambda:12.499395786804916 rho:0.19644613298109947\n",
      "Batch total iterations: 136, parameters: mu:0.005462370149052378 lambda:12.499395786804916 rho:0.19644613298109947\n",
      "Batch total iterations: 99, parameters: mu:0.005090270800805291 lambda:12.499226178753306 rho:0.23555315744994648\n",
      "Batch total iterations: 73, parameters: mu:0.0039047366498188517 lambda:12.49904723336919 rho:0.2734717783717215\n",
      "Batch total iterations: 75, parameters: mu:0.0039047366498188517 lambda:12.49904723336919 rho:0.2734717783717215\n",
      "Batch total iterations: 74, parameters: mu:0.0039047366498188517 lambda:12.49904723336919 rho:0.2734717783717215\n",
      "Batch total iterations: 57, parameters: mu:0.0022618380134443467 lambda:12.498856369343374 rho:0.3102473304357082\n",
      "Batch total iterations: 45, parameters: mu:0.0008839797735768976 lambda:12.498656832644933 rho:0.3455164780057604\n",
      "Batch total iterations: 46, parameters: mu:0.0008839797735768976 lambda:12.498656832644933 rho:0.3455164780057604\n",
      "Batch total iterations: 46, parameters: mu:0.0008839797735768976 lambda:12.498656832644933 rho:0.3455164780057604\n",
      "Batch total iterations: 37, parameters: mu:0.00045379689047912557 lambda:12.498451427141791 rho:0.3790894677080528\n",
      "Batch total iterations: 32, parameters: mu:0.0013014374655681673 lambda:12.49824620867447 rho:0.41065190364339227\n",
      "Batch total iterations: 34, parameters: mu:0.0013014374655681673 lambda:12.49824620867447 rho:0.41065190364339227\n",
      "Batch total iterations: 33, parameters: mu:0.0013014374655681673 lambda:12.49824620867447 rho:0.41065190364339227\n",
      "Batch total iterations: 29, parameters: mu:0.003668652335920221 lambda:12.498044390188642 rho:0.44014506285231514\n",
      "Batch total iterations: 26, parameters: mu:0.007368566942564512 lambda:12.497849303922862 rho:0.4675166545677345\n",
      "Batch total iterations: 28, parameters: mu:0.007368566942564512 lambda:12.497849303922862 rho:0.4675166545677345\n",
      "Epoch 10, Train loss 0.01015652, Validation loss 0.00791859\n",
      "Batch total iterations: 27, parameters: mu:0.007368566942564512 lambda:12.497849303922862 rho:0.4675166545677345\n",
      "Batch total iterations: 26, parameters: mu:0.012259682874942794 lambda:12.497662803198164 rho:0.4928211953817419\n",
      "Batch total iterations: 40, parameters: mu:0.017989604300671716 lambda:12.497486666338041 rho:0.5161018567178828\n",
      "Batch total iterations: 37, parameters: mu:0.017989604300671716 lambda:12.497486666338041 rho:0.5161018567178828\n",
      "Batch total iterations: 41, parameters: mu:0.017989604300671716 lambda:12.497486666338041 rho:0.5161018567178828\n",
      "Batch total iterations: 53, parameters: mu:0.02412569441136641 lambda:12.497319355984821 rho:0.5375375263574725\n",
      "Batch total iterations: 60, parameters: mu:0.02997209347892176 lambda:12.497159785132176 rho:0.5572724723355735\n",
      "Batch total iterations: 59, parameters: mu:0.02997209347892176 lambda:12.497159785132176 rho:0.5572724723355735\n",
      "Batch total iterations: 60, parameters: mu:0.02997209347892176 lambda:12.497159785132176 rho:0.5572724723355735\n",
      "Batch total iterations: 60, parameters: mu:0.03530332035857204 lambda:12.497007188610757 rho:0.5754504968367319\n",
      "Batch total iterations: 62, parameters: mu:0.0401108214404648 lambda:12.49686202831877 rho:0.5921605588049239\n",
      "Batch total iterations: 60, parameters: mu:0.0401108214404648 lambda:12.49686202831877 rho:0.5921605588049239\n",
      "Batch total iterations: 62, parameters: mu:0.0401108214404648 lambda:12.49686202831877 rho:0.5921605588049239\n",
      "Batch total iterations: 60, parameters: mu:0.04439361555036978 lambda:12.496723872378128 rho:0.6075238352905922\n",
      "Batch total iterations: 61, parameters: mu:0.048225779056940025 lambda:12.496593184870898 rho:0.6216186452581736\n",
      "Batch total iterations: 60, parameters: mu:0.048225779056940025 lambda:12.496593184870898 rho:0.6216186452581736\n",
      "Batch total iterations: 61, parameters: mu:0.048225779056940025 lambda:12.496593184870898 rho:0.6216186452581736\n",
      "Batch total iterations: 59, parameters: mu:0.051632176318492075 lambda:12.49646954789193 rho:0.6345514469108842\n",
      "Batch total iterations: 60, parameters: mu:0.05468364304823369 lambda:12.496353287103968 rho:0.6463937727123291\n",
      "Batch total iterations: 59, parameters: mu:0.05468364304823369 lambda:12.496353287103968 rho:0.6463937727123291\n",
      "Batch total iterations: 60, parameters: mu:0.05468364304823369 lambda:12.496353287103968 rho:0.6463937727123291\n",
      "Batch total iterations: 58, parameters: mu:0.05740588378944914 lambda:12.496243943880181 rho:0.6572394999966323\n",
      "Batch total iterations: 58, parameters: mu:0.05985559822416712 lambda:12.496141822548292 rho:0.667148406755667\n",
      "Batch total iterations: 57, parameters: mu:0.05985559822416712 lambda:12.496141822548292 rho:0.667148406755667\n",
      "Batch total iterations: 59, parameters: mu:0.05985559822416712 lambda:12.496141822548292 rho:0.667148406755667\n",
      "Batch total iterations: 57, parameters: mu:0.0620488897888055 lambda:12.496046409159849 rho:0.6762033960662815\n",
      "Batch total iterations: 58, parameters: mu:0.06403672780730077 lambda:12.495957823551484 rho:0.6844602938290497\n",
      "Batch total iterations: 56, parameters: mu:0.06403672780730077 lambda:12.495957823551484 rho:0.6844602938290497\n",
      "Batch total iterations: 58, parameters: mu:0.06403672780730077 lambda:12.495957823551484 rho:0.6844602938290497\n",
      "Batch total iterations: 56, parameters: mu:0.06583508755042158 lambda:12.495875626875328 rho:0.6919885083756462\n",
      "Batch total iterations: 56, parameters: mu:0.06747760672112063 lambda:12.495799919895834 rho:0.6988340032248951\n",
      "Batch total iterations: 55, parameters: mu:0.06747760672112063 lambda:12.495799919895834 rho:0.6988340032248951\n",
      "Batch total iterations: 56, parameters: mu:0.06747760672112063 lambda:12.495799919895834 rho:0.6988340032248951\n",
      "Batch total iterations: 55, parameters: mu:0.06898687326028285 lambda:12.495730327108543 rho:0.7050552821890234\n",
      "Batch total iterations: 55, parameters: mu:0.07037552981933184 lambda:12.49566668072586 rho:0.7106984759509514\n",
      "Batch total iterations: 55, parameters: mu:0.07037552981933184 lambda:12.49566668072586 rho:0.7106984759509514\n",
      "Batch total iterations: 56, parameters: mu:0.07037552981933184 lambda:12.49566668072586 rho:0.7106984759509514\n",
      "Batch total iterations: 54, parameters: mu:0.07165292789742919 lambda:12.495608604181756 rho:0.7158130447857829\n",
      "Batch total iterations: 55, parameters: mu:0.07284874970941654 lambda:12.495556025734512 rho:0.720436332142523\n",
      "Batch total iterations: 54, parameters: mu:0.07284874970941654 lambda:12.495556025734512 rho:0.720436332142523\n",
      "Epoch 20, Train loss 0.00231712, Validation loss 0.00232815\n",
      "Batch total iterations: 55, parameters: mu:0.07284874970941654 lambda:12.495556025734512 rho:0.720436332142523\n",
      "Batch total iterations: 54, parameters: mu:0.0739626951212722 lambda:12.495508638080263 rho:0.7246083277702936\n",
      "Batch total iterations: 54, parameters: mu:0.07501117069353229 lambda:12.495466083585223 rho:0.7283689775962313\n",
      "Batch total iterations: 53, parameters: mu:0.07501117069353229 lambda:12.495466083585223 rho:0.7283689775962313\n",
      "Batch total iterations: 54, parameters: mu:0.07501117069353229 lambda:12.495466083585223 rho:0.7283689775962313\n",
      "Batch total iterations: 53, parameters: mu:0.07600109324237084 lambda:12.495428229606183 rho:0.7317476219412478\n",
      "Batch total iterations: 54, parameters: mu:0.07694211948592578 lambda:12.495394744865479 rho:0.734778157706951\n",
      "Batch total iterations: 53, parameters: mu:0.07694211948592578 lambda:12.495394744865479 rho:0.734778157706951\n",
      "Batch total iterations: 54, parameters: mu:0.07694211948592578 lambda:12.495394744865479 rho:0.734778157706951\n",
      "Batch total iterations: 52, parameters: mu:0.0778347503771068 lambda:12.495365387582964 rho:0.737488433647104\n",
      "Batch total iterations: 53, parameters: mu:0.07869409088400277 lambda:12.495339988380792 rho:0.7399032951068346\n",
      "Batch total iterations: 52, parameters: mu:0.07869409088400277 lambda:12.495339988380792 rho:0.7399032951068346\n",
      "Batch total iterations: 53, parameters: mu:0.07869409088400277 lambda:12.495339988380792 rho:0.7399032951068346\n",
      "Batch total iterations: 52, parameters: mu:0.07951937588186636 lambda:12.495318342803557 rho:0.742045563218399\n",
      "Batch total iterations: 53, parameters: mu:0.08031433168206475 lambda:12.49530006192168 rho:0.7439431613136224\n",
      "Batch total iterations: 51, parameters: mu:0.08031433168206475 lambda:12.49530006192168 rho:0.7439431613136224\n",
      "Batch total iterations: 52, parameters: mu:0.08031433168206475 lambda:12.49530006192168 rho:0.7439431613136224\n",
      "Batch total iterations: 52, parameters: mu:0.08108681460935616 lambda:12.49528502683945 rho:0.7456138841456551\n",
      "Batch total iterations: 52, parameters: mu:0.08182948498402533 lambda:12.495272962700245 rho:0.7470778787246108\n",
      "Batch total iterations: 51, parameters: mu:0.08182948498402533 lambda:12.495272962700245 rho:0.7470778787246108\n",
      "Batch total iterations: 52, parameters: mu:0.08182948498402533 lambda:12.495272962700245 rho:0.7470778787246108\n",
      "Batch total iterations: 51, parameters: mu:0.08255452915866304 lambda:12.49526379114603 rho:0.7483494800379088\n",
      "Batch total iterations: 52, parameters: mu:0.0832530061201565 lambda:12.495257085687792 rho:0.7494518279248047\n",
      "Batch total iterations: 51, parameters: mu:0.0832530061201565 lambda:12.495257085687792 rho:0.7494518279248047\n",
      "Batch total iterations: 52, parameters: mu:0.0832530061201565 lambda:12.495257085687792 rho:0.7494518279248047\n",
      "Batch total iterations: 51, parameters: mu:0.08392633736853125 lambda:12.495252701483215 rho:0.7503982958061819\n",
      "Batch total iterations: 51, parameters: mu:0.08458283013820841 lambda:12.495250551887585 rho:0.751200302951279\n",
      "Batch total iterations: 51, parameters: mu:0.08458283013820841 lambda:12.495250551887585 rho:0.751200302951279\n",
      "Batch total iterations: 52, parameters: mu:0.08458283013820841 lambda:12.495250551887585 rho:0.751200302951279\n",
      "Batch total iterations: 50, parameters: mu:0.0852192675396634 lambda:12.495250385159554 rho:0.7518727447979928\n",
      "Batch total iterations: 51, parameters: mu:0.08583921576755749 lambda:12.495252104503017 rho:0.7524257498328574\n",
      "Batch total iterations: 50, parameters: mu:0.08583921576755749 lambda:12.495252104503017 rho:0.7524257498328574\n",
      "Batch total iterations: 51, parameters: mu:0.08583921576755749 lambda:12.495252104503017 rho:0.7524257498328574\n",
      "Batch total iterations: 50, parameters: mu:0.08643659675859178 lambda:12.495255427970603 rho:0.7528736840658573\n",
      "Batch total iterations: 51, parameters: mu:0.08702446768470117 lambda:12.495260405712603 rho:0.7532209838369688\n",
      "Batch total iterations: 50, parameters: mu:0.08702446768470117 lambda:12.495260405712603 rho:0.7532209838369688\n",
      "Batch total iterations: 51, parameters: mu:0.08702446768470117 lambda:12.495260405712603 rho:0.7532209838369688\n",
      "Batch total iterations: 50, parameters: mu:0.08758773374855189 lambda:12.495266690389304 rho:0.7534826921597472\n",
      "Batch total iterations: 51, parameters: mu:0.08814241161395847 lambda:12.49527433054497 rho:0.7536620317412763\n",
      "Batch total iterations: 50, parameters: mu:0.08814241161395847 lambda:12.49527433054497 rho:0.7536620317412763\n",
      "Epoch 30, Train loss 0.00232709, Validation loss 0.00235468\n",
      "Batch total iterations: 51, parameters: mu:0.08814241161395847 lambda:12.49527433054497 rho:0.7536620317412763\n",
      "Batch total iterations: 50, parameters: mu:0.08867548203808193 lambda:12.495283089055885 rho:0.7537698383713546\n",
      "Batch total iterations: 50, parameters: mu:0.08919568629449867 lambda:12.495292864791814 rho:0.7538132068231329\n",
      "Batch total iterations: 50, parameters: mu:0.08919568629449867 lambda:12.495292864791814 rho:0.7538132068231329\n",
      "Batch total iterations: 50, parameters: mu:0.08919568629449867 lambda:12.495292864791814 rho:0.7538132068231329\n",
      "Batch total iterations: 50, parameters: mu:0.0897058318212625 lambda:12.495303784516043 rho:0.7537915088819561\n",
      "Batch total iterations: 50, parameters: mu:0.09019400071507377 lambda:12.495315261692381 rho:0.7537255367995953\n",
      "Batch total iterations: 50, parameters: mu:0.09019400071507377 lambda:12.495315261692381 rho:0.7537255367995953\n",
      "Batch total iterations: 50, parameters: mu:0.09019400071507377 lambda:12.495315261692381 rho:0.7537255367995953\n",
      "Batch total iterations: 50, parameters: mu:0.0906679890626996 lambda:12.495327441786866 rho:0.7536137081333428\n",
      "Batch total iterations: 50, parameters: mu:0.09112329366466361 lambda:12.495340239995777 rho:0.7534603908272904\n",
      "Batch total iterations: 50, parameters: mu:0.09112329366466361 lambda:12.495340239995777 rho:0.7534603908272904\n",
      "Batch total iterations: 50, parameters: mu:0.09112329366466361 lambda:12.495340239995777 rho:0.7534603908272904\n",
      "Batch total iterations: 50, parameters: mu:0.09156473542539174 lambda:12.495353720776102 rho:0.7532660961716842\n",
      "Batch total iterations: 50, parameters: mu:0.09198706154252073 lambda:12.495367482810526 rho:0.7530448842725647\n",
      "Batch total iterations: 49, parameters: mu:0.09198706154252073 lambda:12.495367482810526 rho:0.7530448842725647\n",
      "Batch total iterations: 50, parameters: mu:0.09198706154252073 lambda:12.495367482810526 rho:0.7530448842725647\n",
      "Batch total iterations: 49, parameters: mu:0.09239656950854101 lambda:12.495381762066051 rho:0.7527914832885202\n",
      "Batch total iterations: 50, parameters: mu:0.0927893932884399 lambda:12.495396240371132 rho:0.7525172995418496\n",
      "Batch total iterations: 49, parameters: mu:0.0927893932884399 lambda:12.495396240371132 rho:0.7525172995418496\n",
      "Batch total iterations: 50, parameters: mu:0.0927893932884399 lambda:12.495396240371132 rho:0.7525172995418496\n",
      "Batch total iterations: 49, parameters: mu:0.09316365253723587 lambda:12.495410844680377 rho:0.7522258468344646\n",
      "Batch total iterations: 50, parameters: mu:0.09352797516264484 lambda:12.495425827051221 rho:0.751910774161376\n",
      "Batch total iterations: 49, parameters: mu:0.09352797516264484 lambda:12.495425827051221 rho:0.751910774161376\n",
      "Batch total iterations: 50, parameters: mu:0.09352797516264484 lambda:12.495425827051221 rho:0.751910774161376\n",
      "Batch total iterations: 49, parameters: mu:0.09387754695477789 lambda:12.495441115976623 rho:0.751575034879108\n",
      "Batch total iterations: 50, parameters: mu:0.09421299380856453 lambda:12.495456353476241 rho:0.7512312435834898\n",
      "Batch total iterations: 49, parameters: mu:0.09421299380856453 lambda:12.495456353476241 rho:0.7512312435834898\n",
      "Batch total iterations: 50, parameters: mu:0.09421299380856453 lambda:12.495456353476241 rho:0.7512312435834898\n",
      "Batch total iterations: 49, parameters: mu:0.0945356716010644 lambda:12.49547168615718 rho:0.7508755583357543\n",
      "Batch total iterations: 50, parameters: mu:0.09484210940760726 lambda:12.495487002686874 rho:0.7505120914333563\n",
      "Batch total iterations: 49, parameters: mu:0.09484210940760726 lambda:12.495487002686874 rho:0.7505120914333563\n",
      "Batch total iterations: 50, parameters: mu:0.09484210940760726 lambda:12.495487002686874 rho:0.7505120914333563\n",
      "Batch total iterations: 49, parameters: mu:0.09513294218722626 lambda:12.495502213456929 rho:0.7501442895362975\n",
      "Batch total iterations: 49, parameters: mu:0.09541171381994763 lambda:12.495517478925985 rho:0.7497682259050947\n",
      "Batch total iterations: 49, parameters: mu:0.09541171381994763 lambda:12.495517478925985 rho:0.7497682259050947\n",
      "Batch total iterations: 50, parameters: mu:0.09541171381994763 lambda:12.495517478925985 rho:0.7497682259050947\n",
      "Batch total iterations: 49, parameters: mu:0.09567472585471477 lambda:12.49553262285942 rho:0.7493895711249948\n",
      "Batch total iterations: 49, parameters: mu:0.0959269001420053 lambda:12.49554771113688 rho:0.7490072802691722\n",
      "Batch total iterations: 49, parameters: mu:0.0959269001420053 lambda:12.49554771113688 rho:0.7490072802691722\n",
      "Epoch 40, Train loss 0.00230583, Validation loss 0.00232980\n",
      "Batch total iterations: 50, parameters: mu:0.0959269001420053 lambda:12.49554771113688 rho:0.7490072802691722\n",
      "Batch total iterations: 49, parameters: mu:0.09616593784680841 lambda:12.495562726668046 rho:0.7486219948710479\n",
      "Batch total iterations: 50, parameters: mu:0.09639286331150826 lambda:12.495577539570542 rho:0.7482385588510875\n",
      "Batch total iterations: 49, parameters: mu:0.09639286331150826 lambda:12.495577539570542 rho:0.7482385588510875\n",
      "Batch total iterations: 50, parameters: mu:0.09639286331150826 lambda:12.495577539570542 rho:0.7482385588510875\n",
      "Batch total iterations: 49, parameters: mu:0.0966084950869321 lambda:12.495592156211185 rho:0.7478570587730797\n",
      "Batch total iterations: 50, parameters: mu:0.09681095692001518 lambda:12.495606622369952 rho:0.7474763194972267\n",
      "Batch total iterations: 49, parameters: mu:0.09681095692001518 lambda:12.495606622369952 rho:0.7474763194972267\n",
      "Batch total iterations: 49, parameters: mu:0.09681095692001518 lambda:12.495606622369952 rho:0.7474763194972267\n",
      "Batch total iterations: 49, parameters: mu:0.0970055743866169 lambda:12.495620909548006 rho:0.7470984008391107\n",
      "Batch total iterations: 49, parameters: mu:0.09718785037779379 lambda:12.4956350179181 rho:0.7467224959296203\n",
      "Batch total iterations: 49, parameters: mu:0.09718785037779379 lambda:12.4956350179181 rho:0.7467224959296203\n",
      "Batch total iterations: 50, parameters: mu:0.09718785037779379 lambda:12.4956350179181 rho:0.7467224959296203\n",
      "Batch total iterations: 49, parameters: mu:0.09736134728111144 lambda:12.495648940828612 rho:0.7463494425843706\n",
      "Batch total iterations: 49, parameters: mu:0.0975221804106493 lambda:12.4956626043013 rho:0.745981829356115\n",
      "Batch total iterations: 49, parameters: mu:0.0975221804106493 lambda:12.4956626043013 rho:0.745981829356115\n",
      "Batch total iterations: 50, parameters: mu:0.0975221804106493 lambda:12.4956626043013 rho:0.745981829356115\n",
      "Batch total iterations: 49, parameters: mu:0.0976691247985478 lambda:12.4956760443703 rho:0.7456184646523709\n",
      "Batch total iterations: 49, parameters: mu:0.09781092368534276 lambda:12.495689240015523 rho:0.745260872580431\n",
      "Batch total iterations: 49, parameters: mu:0.09781092368534276 lambda:12.495689240015523 rho:0.745260872580431\n",
      "Batch total iterations: 50, parameters: mu:0.09781092368534276 lambda:12.495689240015523 rho:0.745260872580431\n",
      "Batch total iterations: 48, parameters: mu:0.09793949568443262 lambda:12.49570209455556 rho:0.7449118651355161\n",
      "Batch total iterations: 49, parameters: mu:0.09806665421344259 lambda:12.495714868211518 rho:0.7445641067159111\n",
      "Batch total iterations: 49, parameters: mu:0.09806665421344259 lambda:12.495714868211518 rho:0.7445641067159111\n",
      "Batch total iterations: 49, parameters: mu:0.09806665421344259 lambda:12.495714868211518 rho:0.7445641067159111\n",
      "Batch total iterations: 49, parameters: mu:0.09818569318259555 lambda:12.495727286408753 rho:0.744226237346896\n",
      "Batch total iterations: 49, parameters: mu:0.09829729465564326 lambda:12.495739597054312 rho:0.7438900385883431\n",
      "Batch total iterations: 49, parameters: mu:0.09829729465564326 lambda:12.495739597054312 rho:0.7438900385883431\n",
      "Batch total iterations: 49, parameters: mu:0.09829729465564326 lambda:12.495739597054312 rho:0.7438900385883431\n",
      "Batch total iterations: 49, parameters: mu:0.09840080000021194 lambda:12.495751625832419 rho:0.7435616086968165\n",
      "Batch total iterations: 49, parameters: mu:0.0984979662809742 lambda:12.495763455490483 rho:0.743238067304461\n",
      "Batch total iterations: 49, parameters: mu:0.0984979662809742 lambda:12.495763455490483 rho:0.743238067304461\n",
      "Batch total iterations: 49, parameters: mu:0.0984979662809742 lambda:12.495763455490483 rho:0.743238067304461\n",
      "Batch total iterations: 49, parameters: mu:0.09859124675910998 lambda:12.495775150567843 rho:0.7429178388715769\n",
      "Batch total iterations: 49, parameters: mu:0.0986745607029864 lambda:12.495786482938232 rho:0.7426078991390244\n",
      "Batch total iterations: 49, parameters: mu:0.0986745607029864 lambda:12.495786482938232 rho:0.7426078991390244\n",
      "Batch total iterations: 49, parameters: mu:0.0986745607029864 lambda:12.495786482938232 rho:0.7426078991390244\n",
      "Batch total iterations: 49, parameters: mu:0.09868215234220268 lambda:12.495787589699995 rho:0.7425776640462757\n",
      "Batch total iterations: 49, parameters: mu:0.09868925173985854 lambda:12.495788676732072 rho:0.7425479533717261\n",
      "Batch total iterations: 49, parameters: mu:0.09868925173985854 lambda:12.495788676732072 rho:0.7425479533717261\n",
      "Epoch 50, Train loss 0.00228924, Validation loss 0.00231426\n",
      "Batch total iterations: 50, parameters: mu:0.09868925173985854 lambda:12.495788676732072 rho:0.7425479533717261\n",
      "Batch total iterations: 49, parameters: mu:0.09869523733403383 lambda:12.495789735929824 rho:0.7425189990773807\n",
      "Batch total iterations: 49, parameters: mu:0.0987011522041627 lambda:12.495790780830063 rho:0.7424904397366262\n",
      "Batch total iterations: 49, parameters: mu:0.0987011522041627 lambda:12.495790780830063 rho:0.7424904397366262\n",
      "Batch total iterations: 49, parameters: mu:0.0987011522041627 lambda:12.495790780830063 rho:0.7424904397366262\n",
      "Batch total iterations: 49, parameters: mu:0.09870646025407101 lambda:12.495791807096035 rho:0.7424624363924788\n",
      "Batch total iterations: 50, parameters: mu:0.09871157823805453 lambda:12.495792820751038 rho:0.7424347537757618\n",
      "Batch total iterations: 48, parameters: mu:0.09871157823805453 lambda:12.495792820751038 rho:0.7424347537757618\n",
      "Batch total iterations: 50, parameters: mu:0.09871157823805453 lambda:12.495792820751038 rho:0.7424347537757618\n",
      "Batch total iterations: 49, parameters: mu:0.09871582658456456 lambda:12.495793818169146 rho:0.7424074944160799\n",
      "Batch total iterations: 49, parameters: mu:0.09872009277868368 lambda:12.495794801064436 rho:0.7423806579773553\n",
      "Batch total iterations: 49, parameters: mu:0.09872009277868368 lambda:12.495794801064436 rho:0.7423806579773553\n",
      "Batch total iterations: 50, parameters: mu:0.09872009277868368 lambda:12.495794801064436 rho:0.7423806579773553\n",
      "Batch total iterations: 49, parameters: mu:0.0987235464784967 lambda:12.49579577313362 rho:0.7423540748329281\n",
      "Batch total iterations: 50, parameters: mu:0.09872711186797149 lambda:12.495796730072293 rho:0.7423279578859072\n",
      "Batch total iterations: 48, parameters: mu:0.09872711186797149 lambda:12.495796730072293 rho:0.7423279578859072\n",
      "Batch total iterations: 49, parameters: mu:0.09872711186797149 lambda:12.495796730072293 rho:0.7423279578859072\n",
      "Batch total iterations: 49, parameters: mu:0.09873067258789438 lambda:12.495797684731624 rho:0.7423019024950157\n",
      "Batch total iterations: 50, parameters: mu:0.09873381285895551 lambda:12.495798624048494 rho:0.7422762969382006\n",
      "Batch total iterations: 48, parameters: mu:0.09873381285895551 lambda:12.495798624048494 rho:0.7422762969382006\n",
      "Batch total iterations: 50, parameters: mu:0.09873381285895551 lambda:12.495798624048494 rho:0.7422762969382006\n",
      "Batch total iterations: 49, parameters: mu:0.09873643835066266 lambda:12.495799541137258 rho:0.7422513604069569\n",
      "Batch total iterations: 49, parameters: mu:0.098739075009538 lambda:12.495800463328653 rho:0.7422262215694868\n",
      "Batch total iterations: 49, parameters: mu:0.098739075009538 lambda:12.495800463328653 rho:0.7422262215694868\n",
      "Batch total iterations: 49, parameters: mu:0.098739075009538 lambda:12.495800463328653 rho:0.7422262215694868\n",
      "Batch total iterations: 49, parameters: mu:0.09874220038554342 lambda:12.495801402822735 rho:0.7422005531977655\n",
      "Batch total iterations: 49, parameters: mu:0.0987445264681636 lambda:12.495802310942667 rho:0.7421758347328139\n",
      "Batch total iterations: 49, parameters: mu:0.0987445264681636 lambda:12.495802310942667 rho:0.7421758347328139\n",
      "Batch total iterations: 50, parameters: mu:0.0987445264681636 lambda:12.495802310942667 rho:0.7421758347328139\n",
      "Batch total iterations: 49, parameters: mu:0.09874661834085473 lambda:12.495803212849466 rho:0.7421512807696251\n",
      "Batch total iterations: 49, parameters: mu:0.09874855183706174 lambda:12.495804107303458 rho:0.7421269383500729\n",
      "Batch total iterations: 48, parameters: mu:0.09874855183706174 lambda:12.495804107303458 rho:0.7421269383500729\n",
      "Batch total iterations: 50, parameters: mu:0.09874855183706174 lambda:12.495804107303458 rho:0.7421269383500729\n",
      "Batch total iterations: 49, parameters: mu:0.09874988804929072 lambda:12.49580499680719 rho:0.7421026901139481\n",
      "Batch total iterations: 49, parameters: mu:0.09875153192967193 lambda:12.495805879336125 rho:0.7420786830761\n",
      "Batch total iterations: 49, parameters: mu:0.09875153192967193 lambda:12.495805879336125 rho:0.7420786830761\n"
     ]
    }
   ],
   "source": [
    "max_iter, epochs = 1000, 60\n",
    "\n",
    "# Train and apply L-ADMM One Parameter with T iterations / layers. b_x, b_s are a batch from the validation set\n",
    "ladmm_mse, admm1d_model, b_x, b_s = admm_1d_apply(train_loader, test_loader, max_iter, H)\n",
    "\n",
    "admm_mse = admm_apply(test_loader, max_iter, H)\n",
    "\n",
    "######################### Visuallization #########################\n",
    "b_x, s_gt = b_x[0], b_s[0]\n",
    "s_hat_ladmm = admm1d_model(b_x)\n",
    "s_hat_ladmm = s_hat_ladmm.detach().numpy()[0]\n",
    "\n",
    "s_hat_admm = vanilla_admm(x=b_x, H=H, max_itr=max_iter)\n",
    "\n",
    "plot_admm_vs_admm_1d_reconstruction(s_hat_admm=s_hat_admm,\n",
    "                                  s_hat_ladmm=s_hat_ladmm, max_iter=max_iter,s_gt = s_gt, epochs=epochs)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_admm_vs_admm_1d_reconstruction(s_hat_admm=s_hat_admm,\n",
    "                                    s_hat_ladmm=s_hat_ladmm,\n",
    "                                    max_iter=max_iter, s_gt = s_gt, epochs=epochs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
